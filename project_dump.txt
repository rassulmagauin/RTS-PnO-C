==================================================
FILE: ./README.md
==================================================
# RTS-PnO
Risk-aware Fund Allocation based on Time-Series Forcasting

### Run PnO Experiment
```
  python src/run_pno.py --config PNO_config
```

### Run PtO Experiment
First run the prediction stage
```
  python src/run_conformal.py --config Predict_config
```

Then run the optimization stage
```
  python src/run_pto.py --config PtO_config
```

### Run Heuristic Experiment

First run the prediction stage
```
  python src/run_conformal.py --config Predict_config
```

Then run the heuristic allocation stage
```
  python src/run_heuristic.py --config Heuristic_config
```


*We will open-source the config files after acceptance.*


==================================================
FILE: ./LICENSE
==================================================
MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



==================================================
FILE: ./dataset/ETH_1hour.csv
==================================================
(Showing first 10 lines only – dataset truncated)

,Unnamed: 0,Unix Timestamp,Date,Symbol,Open,High,Low,Close,Volume
18,2117767,1595059200,2020-07-18 08:00:00+00:00,ETH-USD,233.3,233.3,233.27,233.28,8.83727713
77,2117826,1595062800,2020-07-18 09:00:00+00:00,ETH-USD,234.4,234.4,234.38,234.38,3.52594936
137,2117886,1595066400,2020-07-18 10:00:00+00:00,ETH-USD,235.08,235.13,235.07,235.13,4.44619879
196,2117945,1595070000,2020-07-18 11:00:00+00:00,ETH-USD,234.87,234.89,234.87,234.89,56.70712271
254,2118003,1595073600,2020-07-18 12:00:00+00:00,ETH-USD,234.95,235.22,234.95,235.22,91.23535395
314,2118063,1595077200,2020-07-18 13:00:00+00:00,ETH-USD,234.87,235.03,234.87,235.02,45.98275822
374,2118123,1595080800,2020-07-18 14:00:00+00:00,ETH-USD,234.65,234.73,234.65,234.66,20.43291213
434,2118183,1595084400,2020-07-18 15:00:00+00:00,ETH-USD,234.97,235.0,234.94,235.0,15.91754473
494,2118243,1595088000,2020-07-18 16:00:00+00:00,ETH-USD,235.14,235.19,235.13,235.15,120.27941441



==================================================
FILE: ./dataset/stock_data.csv
==================================================
(Showing first 10 lines only – dataset truncated)

dt,vix,sp500,sp500_volume,djia,djia_volume,hsi,ads,us3m,joblessness,epu,GPRD,prev_day
1990-01-03,18.19,358.76001,192330000.0,2809.73,23.62,2858.699951,-0.229917,7.89,3,100.359178207312,75.4080505371094,359.690002
1990-01-04,19.22,355.670013,177000000.0,2796.08,24.37,2868.0,-0.246065,7.84,3,100.359178207312,56.0858039855957,358.76001
1990-01-05,20.11,352.200012,158530000.0,2773.25,20.29,2839.899902,-0.260393,7.79,3,100.359178207312,63.8476753234863,355.670013
1990-01-08,20.26,353.790009,140110000.0,2794.37,16.61,2816.0,-0.29175,7.79,3,100.359178207312,102.841156005859,352.200012
1990-01-09,22.2,349.619995,155210000.0,2766.0,15.8,2822.0,-0.297326,7.8,3,100.359178207312,138.435668945312,353.790009
1990-01-10,22.44,347.309998,175990000.0,2750.64,19.95,2868.0,-0.300341,7.75,3,100.359178207312,103.517738342285,349.619995
1990-01-11,20.05,348.529999,154390000.0,2760.67,15.79,2855.0,-0.300796,7.8,3,100.359178207312,77.2074661254883,347.309998
1990-01-12,24.64,339.929993,183880000.0,2689.21,24.18,2835.0,-0.298692,7.74,3,100.359178207312,69.9548492431641,348.529999
1990-01-15,26.34,337.0,140590000.0,2669.37,18.63,2788.600098,-0.277353,7.74,3,100.359178207312,101.842697143555,339.929993



==================================================
FILE: ./dataset/exchange_rate_600s.csv
==================================================
(Showing first 10 lines only – dataset truncated)

fdate,sec_in_fdate,transact_time,usdcny,usdrub,usdjpy,usdmyr,usdtwd,usdbdt,usdthb,usdcop,usdtry,usdkwd,audusd,nzdusd,usdphp,usdfjd,usdsgd
20230710,33600,20230710092000,7.2258,91.44809999999998,1.4258176666666666,4.660006666666667,31.30377666666667,109.0,35.086625,4171.569999999999,26.076699999999995,3.0737999999999994,0.6690043333333333,0.6211513333333333,55.558433333333326,2.2437000000000005,1.3468435
20230710,34200,20230710093000,7.222773333333334,91.44809999999998,1.425738,4.661188333333333,31.32537333333333,109.0,35.114266666666666,4171.569999999999,26.076699999999995,3.0737999999999994,0.6696,0.6207290000000001,55.55711666666667,2.2437000000000005,1.3474275
20230710,34800,20230710094000,7.223151333333332,91.44809999999998,1.4248195,4.662796666666666,31.340146666666666,109.0,35.12089166666667,4171.569999999999,26.076699999999995,3.0737999999999994,0.6696,0.6201233333333334,55.556765,2.2437000000000005,1.3477043333333336
20230710,35400,20230710095000,7.224151833333332,91.44809999999998,1.4251356666666668,4.6638475,31.35027666666667,109.0,35.123,4171.569999999999,26.076699999999995,3.0737999999999994,0.6687114999999999,0.6202034999999999,55.55561166666666,2.2437000000000005,1.347627
20230710,36000,20230710100000,7.225059833333333,91.44809999999998,1.4255855,4.665798333333334,31.358753333333333,109.0,35.10915833333334,4171.569999999999,26.076699999999995,3.0737999999999994,0.6689076666666667,0.6205445,55.55,2.244333166666667,1.3473770000000005
20230710,36600,20230710101000,7.226289999999999,91.44809999999998,1.4265815000000002,4.66587,31.358625000000004,109.0,35.112575,4171.569999999999,26.076699999999995,3.0737999999999994,0.6685524999999999,0.6203458333333334,55.55,2.2437000000000005,1.3475968333333332
20230710,37200,20230710102000,7.228479999999999,91.44809999999998,1.427471333333333,4.667738333333333,31.360108333333333,109.0,35.132466666666666,4171.569999999999,26.076699999999995,3.0737999999999994,0.6681696666666668,0.6199935,55.55865000000001,2.2437000000000005,1.3478278333333331
20230710,37800,20230710103000,7.231309999999999,91.44809999999998,1.4278034999999998,4.669069166666667,31.361606666666667,109.0,35.13999999999999,4171.569999999999,26.076699999999995,3.0737999999999994,0.6679545000000001,0.6198625,55.56299333333334,2.2437000000000005,1.3481448333333337
20230710,38400,20230710104000,7.233190000000001,91.44809999999998,1.4283656666666664,4.6700025,31.359103333333337,109.0,35.14425,4171.569999999999,26.076699999999995,3.0737999999999994,0.6678661666666666,0.6196795,55.568470000000005,2.2437000000000005,1.3482705



==================================================
FILE: ./dataset/coinbase_1hour.csv
==================================================
(Showing first 10 lines only – dataset truncated)

,Unnamed: 0,Unix Timestamp,Date,Symbol,Open,High,Low,Close,Volume
10,2455913,1574848800,2019-11-27 10:00:00+00:00,BTC-USD,6892.16,6892.17,6880.01,6882.65,26.14589409
70,2455973,1574852400,2019-11-27 11:00:00+00:00,BTC-USD,6909.1,6915.71,6909.1,6912.9,3.44796747
130,2456033,1574856000,2019-11-27 12:00:00+00:00,BTC-USD,7158.19,7158.19,7138.33,7143.47,22.44413729
190,2456093,1574859600,2019-11-27 13:00:00+00:00,BTC-USD,7236.8,7248.7,7236.8,7248.7,5.17523704
250,2456153,1574863200,2019-11-27 14:00:00+00:00,BTC-USD,7257.84,7273.79,7257.83,7270.94,5.42411967
310,2456213,1574866800,2019-11-27 15:00:00+00:00,BTC-USD,7236.98,7241.82,7236.47,7239.4,5.06585135
370,2456273,1574870400,2019-11-27 16:00:00+00:00,BTC-USD,7315.19,7355.31,7315.19,7338.93,59.59576719
430,2456333,1574874000,2019-11-27 17:00:00+00:00,BTC-USD,7484.18,7530.0,7484.18,7525.0,123.02246337
490,2456393,1574877600,2019-11-27 18:00:00+00:00,BTC-USD,7575.99,7599.0,7575.98,7599.0,15.406302900000002



==================================================
FILE: ./.gitignore
==================================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# PyPI configuration file
.pypirc

.DS_Store
*/.DS_Store
*.ipynb


==================================================
FILE: ./core/summary_usdcny_old.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 0.0034386831617327076 | 0.0004754966945323532 | 0.0060256170108914375 | 0.05091505125164986
0.2      | 0.003320800402741224 | 0.0004592327139684572 | 0.0060256170108914375 | 0.05091505125164986
0.25     | 0.003285269277900379 | 0.0004543204776192754 | 0.0060256170108914375 | 0.05091505125164986
0.3      | 0.0032643240899862614 | 0.0004514244498924644 | 0.0060256170108914375 | 0.05091505125164986
0.4      | 0.0032360863634844556 | 0.00044751889665177273 | 0.0060256170108914375 | 0.05091505125164986
0.5      | 0.003219990084704034 | 0.0004452927674313593 | 0.0060256170108914375 | 0.05091505125164986
0.6      | 0.0032299580821608985 | 0.00044667441580615685 | 0.0060256170108914375 | 0.05091505125164986
0.75     | 0.003243180249916217 | 0.00044850787918513144 | 0.0060256170108914375 | 0.05091505125164986
0.7      | 0.003239061775853553 | 0.00044793663190485203 | 0.0060256170108914375 | 0.05091505125164986
0.8      | 0.0032469975813501635 | 0.00044903752697811593 | 0.0060256170108914375 | 0.05091505125164986
0.9      | 0.0032545306800996904 | 0.0004500829058414167 | 0.0060256170108914375 | 0.05091505125164986
1.0      | 0.0032625500660816427 | 0.00045119614023214367 | 0.0060256170108914375 | 0.05091505125164986



==================================================
FILE: ./core/src/allocate/data_provider.py
==================================================
import os
import warnings

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from pyepo.data.dataset import optDataset
from torch.utils.data import DataLoader

def get_allocating_loader_and_dataset(configs, optmodel, split, shuffle, interval_override=None):
    dataset = Allocatedataset(
        configs=configs, 
        optmodel=optmodel, 
        interval=configs.interval if interval_override is None else interval_override,
        split=split
    )
    loader = DataLoader(
        dataset.dataset, 
        batch_size=configs.batch_size, 
        shuffle=shuffle,
        num_workers=configs.num_workers
    )
    return loader, dataset

class Allocatedataset:
    def __init__(self, configs, optmodel, interval, split='train', date_col=['fdate', 'sec_in_fdate']):
        self.data_path = os.path.join(configs.root_path, configs.data_path)
        self.seq_len = configs.seq_len
        self.label_len = configs.label_len
        self.pred_len = configs.pred_len
        self.interval = interval
        self.split = split
        
        self.task_type = configs.task_type
        self.univariate = configs.univariate

        self.date_col = configs.date_col
        self.standardize = configs.standardize

        self._load_data()

        data_x, data_y = [], []
        length = len(self)
        print(split, ":", length)
        for index in range(length):
            x, y = self._extract(index)
            data_x.append(x.squeeze())
            data_y.append(y.squeeze())

        data_x, data_y = np.array(data_x), np.array(data_y)
        self.dataset = optDataset(optmodel, data_x, data_y)

    def _load_data(self):
        df_raw = pd.read_csv(self.data_path)
        assert self.date_col in df_raw.columns
        assert (self.task_type == 'M' and self.univariate is None) or \
               (self.task_type == 'U' and self.univariate in df_raw.columns)

        num_train = int(len(df_raw) * 0.7)
        num_test = int(len(df_raw) * 0.2)
        num_val = len(df_raw) - num_train - num_test

        train_end = num_train
        val_end = train_end + num_val
        test_end = val_end + num_test
        start, end = {
            'train': (0, train_end),
            'val': (train_end - self.seq_len, val_end),
            'test': (val_end - self.seq_len, test_end)
        }[self.split]

        # self.data = df_raw.drop(['fdate', 'sec_in_fdate', 'transact_time'], axis=1)
        df_data = df_raw[[self.univariate]]

        if self.standardize:
            self.scaler = StandardScaler()
            self.scaler.fit(df_data[:num_train].values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        df_stamp = df_raw[self.date_col]
        stamp = df_stamp

        self.data = data[start:end]
        self.stamp = stamp[start:end]
        self.num_vars = self.data.shape[1]

    def __len__(self):
        return (len(self.data)-self.seq_len-self.pred_len)//self.interval + 1
    
    def _extract(self, index):
        if index < 0 or index >= len(self):
            raise IndexError('negative index or index out of range')

        x_start = index * self.interval
        x_end = x_start + self.seq_len
        x_data = self.data[x_start:x_end].astype(np.float32)
        
        y_start = x_end
        y_end = y_start + self.pred_len
        y_data = self.data[y_start:y_end].astype(np.float32)

        return x_data, y_data
    
    def inverse_transform(self, data):
        if not self.standardize:
            warnings.warn('Dataset not standardized, returning as is')
            return data

        return self.scaler.inverse_transform(data)



==================================================
FILE: ./core/src/allocate/experiment_pto.py
==================================================
import os, sys
import json
import time

import numpy as np
import pyepo
import pyepo.metric
import torch
import torch.nn as nn
import torch.optim as optim

from .data_provider import get_allocating_loader_and_dataset
# sys.path.append(os.path.realpath('.'))
import models
from models.Allocate import AllocateModel
from common.experiment import Experiment

class PtOExperiment(Experiment):
    def __init__(self, configs):
        super().__init__(configs)
        self.prev_exp_dir = os.path.join('output', configs.prev_exp_id)
        self._build_forecast_model() # Maybe we should load the pretrained model in the first step
        self._load_constraint()
        self._build_allocate_model()
        self._build_dataloaders()
    
    def _build_dataloaders(self):
        self.test_loader, self.test_set = get_allocating_loader_and_dataset(self.configs, self.allocate_model, split='test', shuffle=False)

    def _build_forecast_model(self):
        self.forecast_model = getattr(models, self.configs.model)(self.configs)
        
        if self.configs.use_multi_gpu:
            self.forecast_model = nn.DataParallel(self.forecast_model, device_ids=self.configs.gpus)

        self.forecast_model.to(self.device)

        self.model = self.forecast_model # Make it compatible with common experiment
        if self.configs.load_prev_weights:
            self.load_checkpoint(model_path=os.path.join(self.prev_exp_dir, 'model.pt'))

    def _build_allocate_model(self):
        self.allocate_model = AllocateModel(self.pretrained_constraint, self.configs.uncertainty_quantile)

    def _load_constraint(self):
        constraint_path = os.path.join(self.prev_exp_dir, 'score.pt')
        self.pretrained_constraint = torch.transpose(torch.load(constraint_path), 0, 1)
       
    @torch.no_grad()
    def evaluate_forecast(self, eval_loader, criterion=None, load_best=False):
        if load_best:
            self._load_best_checkpoint()
        
        self.forecast_model.eval()

        eval_loss, pred, true = [], [], []
        for batch in eval_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_x, batch_y, action, optimal_value = batch
            batch_y_pred = self.forecast_model(batch_x)
            if criterion is not None:
                loss = criterion(batch_y_pred, batch_y)
                eval_loss.append(loss.item())
            
            pred.append(batch_y_pred.cpu().numpy())
            true.append(batch_y.cpu().numpy())
        
        if criterion is not None:
            eval_loss = np.mean(eval_loss)
        else:
            eval_loss = None

        pred = np.concatenate(pred)
        true = np.concatenate(true)

        eval_mse = np.mean((pred -  true) ** 2).item()
        eval_mae = np.mean(np.abs(pred - true)).item()
        eval_metrics = {'MSE': eval_mse, 'MAE': eval_mae}
        
        return eval_loss, eval_metrics
    
    def _cal_regret(self, allocate_model, y_pred, y):
        optimal_cost = np.min(y)
        # opt sol for pred cost
        allocate_model.setObj(y_pred)
        sol, _ = allocate_model.solve()
        # obj with true cost
        sol_cost = np.dot(sol, y)
        if allocate_model.modelSense == pyepo.EPO.MINIMIZE:
            loss = sol_cost - optimal_cost
        if allocate_model.modelSense == pyepo.EPO.MAXIMIZE:
            loss = optimal_cost - sol_cost
        return loss
    
    @torch.no_grad()
    def evaluate_regret(self, eval_loader, scaler=None):
        self.forecast_model.eval()
        total_regret = 0
        abs_regrets = []
        total_rel_regret = 0
        rel_regrets = []
        optsum = 0
        trial = 0

        for batch in eval_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_x, batch_y, optimal_action, optimal_value = batch

            # Predict
            with torch.no_grad():
                batch_y_pred = self.forecast_model(batch_x).to("cpu").detach().numpy()
                batch_y = batch_y.to("cpu").detach().numpy()
                optimal_value = optimal_value.to("cpu").detach().numpy()

            # Different from pyepo
            # Inverse Transform data
            if scaler is not None:
                batch_y_pred = scaler.inverse_transform(batch_y_pred)
                batch_y = scaler.inverse_transform(batch_y)
                optimal_value = scaler.inverse_transform(optimal_value)

            # Solve
            for j in range(batch_y_pred.shape[0]):
                this_regret = self._cal_regret(self.allocate_model, batch_y_pred[j], batch_y[j])
                total_regret += this_regret
                abs_regrets.append(this_regret)
                this_rel_regret = this_regret / np.min(batch_y[j])
                total_rel_regret += this_rel_regret
                rel_regrets.append(this_rel_regret)

            optsum += abs(optimal_value).sum().item()
            trial += batch_x.shape[0]

        return total_regret / trial, abs_regrets, total_rel_regret / trial, rel_regrets
    
    @torch.no_grad()
    def evaluate(self, eval_loader, scaler, criterion=None, save_result=False):
        eval_regret, eval_regrets, eval_rel_regret, eval_rel_regrets = self.evaluate_regret(eval_loader, scaler)
        _, eval_metrics = self.evaluate_forecast(eval_loader, criterion)
        eval_metrics['Regret'] = eval_regret
        eval_metrics['Regrets'] = eval_regrets
        eval_metrics['Rel Regret'] = eval_rel_regret
        eval_metrics['Rel Regrets'] = eval_rel_regrets

        if save_result:
            self._save_results(eval_metrics)

        return eval_metrics
    
    def _save_results(self, metrics):
        res_path = os.path.join(self.exp_dir, 'result.json')
        with open(res_path, 'w') as fout:
            json.dump(metrics, fout, indent=4)



==================================================
FILE: ./core/src/allocate/experiment_pno.py
==================================================
import os, sys
import json
import time

import numpy as np
import pyepo
import pyepo.metric
import torch
import torch.nn as nn
import torch.optim as optim

from .data_provider import get_allocating_loader_and_dataset
# sys.path.append(os.path.realpath('.'))
import models
from models.Allocate import AllocateModel
from common.experiment import Experiment, EarlyStopping
from utils.case_logger import CaseLogger

class PnOExperiment(Experiment):
    def __init__(self, configs):
        super().__init__(configs)
        self.alpha = configs.error_rate
        self.horizon= configs.pred_len
        self._build_forecast_model() # Maybe we should load the pretrained model in the first step
        self._init_constraint()
        self._build_allocate_model()
        self._build_dataloaders()
        if self.configs.data_path != "exchange_rate_600s.csv":
            self._cal_constraint(self.val_loader, self.val_set)
            self._build_allocate_model()

    def _build_dataloaders(self):
        self.train_loader, self.train_set = get_allocating_loader_and_dataset(self.configs, self.allocate_model, split='train', shuffle=True)
        self.val_loader, self.val_set = get_allocating_loader_and_dataset(self.configs, self.allocate_model, split='val', shuffle=False)
        self.test_loader, self.test_set = get_allocating_loader_and_dataset(self.configs, self.allocate_model, split='test', shuffle=False)

    def _build_criterion(self):
        if self.configs.criterion == "SPO+":
            criteria = pyepo.func.SPOPlus(self.allocate_model, processes=self.configs.num_workers) # Maybe the loss function can be added to the MSE (as an ablation)
        elif self.configs.criterion == "MSE+SPO+":
            loss_spo = pyepo.func.SPOPlus(self.allocate_model, processes=self.configs.num_workers)
            loss_mse = nn.MSELoss()
            criteria = lambda y_pred, y, action, value: loss_spo(y_pred, y, action, value) + self.configs.criterion_weight * loss_mse(y_pred, y)
        return criteria

    def _build_forecast_model(self):
        self.forecast_model = getattr(models, self.configs.model)(self.configs)
        
        if self.configs.use_multi_gpu:
            self.forecast_model = nn.DataParallel(self.forecast_model, device_ids=self.configs.gpus)

        self.forecast_model.to(self.device)

        self.model = self.forecast_model # Make it compatible with common experiment

    def _build_allocate_model(self):
        self.allocate_model = AllocateModel(self.constraint, self.configs.uncertainty_quantile)

    def nonconformity(self, pred, true):
        return torch.nn.functional.l1_loss(pred, true, reduction="none")
    
    def _init_constraint(self):
        self.constraint = torch.ones(self.configs.n_vars, self.configs.pred_len) * 100000

    @torch.no_grad()
    def _cal_constraint(self, calib_loader, calib_set):
        self.forecast_model.eval()
        n_calibration = len(calib_set)
        calibration_scores = []
        for batch in calib_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_x, batch_y, optimal_action, optimal_value = batch
            batch_y_pred = self.forecast_model(batch_x)
            score = self.nonconformity(batch_y_pred, batch_y)
            calibration_scores.append(score)

        # [output_size, horizon, n_samples]
        self.calibration_scores = torch.vstack(calibration_scores).transpose(0, 1)

        # [horizon, output_size]
        q = min((n_calibration + 1.0) * (1 - self.alpha) / n_calibration, 1)
        corrected_q = min((n_calibration + 1.0) * (1 - self.alpha / self.horizon) / n_calibration, 1)

        self.critical_calibration_scores = self.get_critical_scores(calibration_scores=self.calibration_scores, q=q)
        self.corrected_critical_calibration_scores = self.get_critical_scores(
            calibration_scores=self.calibration_scores, q=corrected_q
        )

        self.constraint = self.critical_calibration_scores

    def get_critical_scores(self, calibration_scores, q):
        """
        Computes critical calibration scores from scores in the calibration set.

        Args:
            calibration_scores: calibration scores for each example in the
                calibration set.
            q: target quantile for which to return the calibration score

        Returns:
            critical calibration scores for each target horizon
        """

        return torch.tensor(
            [
                [
                    torch.quantile(position_calibration_scores, q=q)
                    for position_calibration_scores in feature_calibration_scores
                ]
                for feature_calibration_scores in calibration_scores
            ]
        ).T
    
    @torch.no_grad()
    def evaluate_forecast(self, eval_loader, criterion=None, load_best=False):
        if load_best:
            self._load_best_checkpoint()
        
        self.forecast_model.eval()

        eval_loss, pred, true = [], [], []
        for batch in eval_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_x, batch_y, action, optimal_value = batch
            batch_y_pred = self.forecast_model(batch_x)
            if criterion is not None:
                loss = criterion(batch_y_pred, batch_y)
                eval_loss.append(loss.item())
            
            pred.append(batch_y_pred.cpu().numpy())
            true.append(batch_y.cpu().numpy())
        
        if criterion is not None:
            eval_loss = np.mean(eval_loss)
        else:
            eval_loss = None

        pred = np.concatenate(pred)
        true = np.concatenate(true)

        eval_mse = np.mean((pred -  true) ** 2).item()
        eval_mae = np.mean(np.abs(pred - true)).item()
        eval_metrics = {'MSE': eval_mse, 'MAE': eval_mae}
        
        return eval_loss, eval_metrics

    @torch.no_grad()
    def _cal_regret(self, allocate_model, y_pred, y):
        optimal_cost = np.min(y)
        # opt sol for pred cost
        allocate_model.setObj(y_pred)
        sol, _ = allocate_model.solve()
        # obj with true cost
        sol_cost = np.dot(sol, y)
        if allocate_model.modelSense == pyepo.EPO.MINIMIZE:
            loss = sol_cost - optimal_cost
        if allocate_model.modelSense == pyepo.EPO.MAXIMIZE:
            loss = optimal_cost - sol_cost
        return loss, sol

    @torch.no_grad()
    def evaluate_regret(self, eval_loader, scaler=None, log_cases=False):
        self.forecast_model.eval()
        loss = 0
        rel_loss = 0
        trial = 0

        # Initialize Logger
        logger = None
        if log_cases:
            cases_dir = os.path.join(self.exp_dir, "cases_test")
            os.makedirs(cases_dir, exist_ok=True)
            # Use 'pno_cases.jsonl' to distinguish from mpc
            logger = CaseLogger(os.path.join(cases_dir, "pno_cases.jsonl"))

        count = 0

        for batch in eval_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_x, batch_y, optimal_action, optimal_value = batch

            # Predict
            with torch.no_grad():
                batch_y_pred = self.forecast_model(batch_x).to("cpu").detach().numpy()
                batch_y = batch_y.to("cpu").detach().numpy()
                optimal_value = optimal_value.to("cpu").detach().numpy()

            # Different from pyepo.metric.regret
            # Inverse Transform data
            if scaler is not None:
                batch_y_pred = scaler.inverse_transform(batch_y_pred)
                batch_y = scaler.inverse_transform(batch_y)
                optimal_value = scaler.inverse_transform(optimal_value)

            # Solve
            for j in range(batch_y_pred.shape[0]):
                # accumulate loss
                this_regret, this_alloc = self._cal_regret(self.allocate_model, batch_y_pred[j], batch_y[j])
                loss += this_regret

                min_val = np.min(batch_y[j])
                this_rel_regret = (this_regret / min_val) if min_val != 0 else 0
                rel_loss += (this_regret / np.min(batch_y[j]))

                if logger:
                    count += 1
                    logger.log({
                        "case_id": count,
                        "algo": "pno",
                        "regret": float(this_regret),
                        "rel_regret": float(this_rel_regret),
                        "alloc": this_alloc.tolist() if isinstance(this_alloc, np.ndarray) else list(this_alloc),
                        "true_prices": batch_y[j].tolist()
                    })

            trial += batch_x.shape[0]

        result = {
            'abs_regret': loss / trial,
            'rel_regret': rel_loss / trial
        }

        return result
        
    @torch.no_grad()
    def evaluate(self, eval_loader, scaler, criterion=None, load_best=False, save_result=False):
        eval_regret = self.evaluate_regret(eval_loader, scaler, log_cases=save_result)
        _, eval_metrics = self.evaluate_forecast(eval_loader, criterion, load_best)
        eval_metrics['Regret'] = eval_regret['abs_regret']
        eval_metrics['Rel Regret'] = eval_regret['rel_regret']

        if save_result:
            self._save_results(eval_metrics)

        return eval_metrics

    def allocate(self):
        criterion = self._build_criterion()
        optimizer, scheduler = self._build_optimizer()
        early_stopping = EarlyStopping(self.configs.patience)

        global_step = 0
        for epoch in range(self.configs.train_epochs):
            self.forecast_model.train()

            train_loss = []
            epoch_start = time.time()
            for batch in self.train_loader:
                optimizer.zero_grad()
                batch = [tensor.to(self.device) for tensor in batch]
                batch_x, batch_y, optimal_action, optimal_value = batch
                batch_y_pred = self.forecast_model(batch_x)

                loss = criterion(batch_y_pred, batch_y, optimal_action, optimal_value)

                loss.backward()
                optimizer.step()

                global_step += 1

                lr = optimizer.param_groups[0]['lr']
                self.writer.add_scalar('B.LR', lr, global_step)

                scheduler.step()
                train_loss.append(loss.item())

            train_loss = np.average(train_loss)
            print(f"Epoch: {epoch + 1} || Training Time: {time.time() - epoch_start:.2f}s")
            print(f"Epoch: {epoch + 1} || Training Loss: {train_loss:.10f}")

            # Update the constraints and rebuild the dataset and allocation model
            self._cal_constraint(self.val_loader, self.val_set)
            self._build_allocate_model()

            val_start = time.time()
            val_metrics = self.evaluate(self.val_loader, self.val_set.scaler)
            val_end = time.time()
            test_metrics = self.evaluate(self.test_loader, self.test_set.scaler)

            print(f"Epoch: {epoch + 1} Val  || Time: {val_end - val_start:.2f}")
            print(f"Epoch: {epoch + 1} Val  || Regret: {val_metrics['Regret']:.8f} || Rel Regret: {val_metrics['Rel Regret']:.8f} || MSE: {val_metrics['MSE']:.4f} || MAE: {val_metrics['MAE']:.4f}")
            print(f"Epoch: {epoch + 1} Test || Regret: {test_metrics['Regret']:.8f} || Rel Regret: {test_metrics['Rel Regret']:.8f} || MSE: {test_metrics['MSE']:.4f} || MAE: {test_metrics['MAE']:.4f}")


            if self.configs.train_epochs > 50:
                early_stopping(val_metrics['Regret'])
                if early_stopping.save_model:
                    self._save_checkpoint()
                    self._save_calib_scores(self.constraint.cpu().detach(), "constraint.pt")
                if early_stopping.early_stop:
                    print("Early stopping")
                    break
            else:
                self._save_checkpoint()
                self._save_calib_scores(self.constraint.cpu().detach(), "constraint.pt")

            self.writer.add_scalar('A.Loss/a.Train', train_loss, global_step)
            self.writer.add_scalar('E.Regret/a.Val', val_metrics['Regret'], global_step)
            self.writer.add_scalar('E.Regret/b.Test', test_metrics['Regret'], global_step)
            self.writer.add_scalar('C.MSE/a.Val', val_metrics['MSE'], global_step)
            self.writer.add_scalar('C.MSE/b.Test', test_metrics['MSE'], global_step)
            self.writer.add_scalar('D.MAE/a.Val', val_metrics['MAE'], global_step)
            self.writer.add_scalar('D.MAE/b.Test', test_metrics['MAE'], global_step)

        self._load_best_checkpoint()
        return self.model

    def _save_results(self, metrics):
        res_path = os.path.join(self.exp_dir, 'result.json')
        with open(res_path, 'w') as fout:
            json.dump(metrics, fout, indent=4)
    
    def _save_calib_scores(self, score, name):
        score_path = os.path.join(self.exp_dir, name)
        torch.save(score, score_path)


==================================================
FILE: ./core/src/allocate/experiment_mpc.py
==================================================
import os
import json
import numpy as np
import torch
import torch.nn as nn
import gurobipy as gp
from tqdm import tqdm

import models
from models.Allocate import AllocateModel
from common.experiment import Experiment
from allocate.data_provider import get_allocating_loader_and_dataset
from utils.case_logger import CaseLogger

class MPCExperiment(Experiment):
    def __init__(self, configs):
        super().__init__(configs)
        self.prev_exp_dir = os.path.join('output', configs.prev_exp_id)
        self._build_forecast_model()
        self._load_constraint()
        self._setup_gurobi_env()
        self._build_allocate_model()

        self.test_loader, self.test_set = get_allocating_loader_and_dataset(
            self.configs, self.allocate_model, split='test', shuffle=False
        )
        self.scaler = self.test_set.scaler

    def _build_forecast_model(self):
        self.forecast_model = getattr(models, self.configs.model)(self.configs)
        if self.configs.use_multi_gpu:
            self.forecast_model = nn.DataParallel(self.forecast_model, device_ids=self.configs.gpus)
        self.forecast_model.to(self.device)
        model_path = os.path.join(self.prev_exp_dir, 'model.pt')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Pre-trained model not found at {model_path}")
        print(f"Loading pre-trained model from: {model_path}")
        self.forecast_model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.forecast_model.eval()
        self.model = self.forecast_model

    def _load_constraint(self):
        constraint_path = os.path.join(self.prev_exp_dir, 'constraint.pt')
        if not os.path.exists(constraint_path):
             constraint_path = os.path.join(self.prev_exp_dir, 'score.pt')
        if not os.path.exists(constraint_path):
             raise FileNotFoundError(f"Constraint file not found at {constraint_path}")
        print(f"Loading constraints from: {constraint_path}")
        self.constraint = torch.load(constraint_path, map_location='cpu')
        if self.constraint.dim() == 1:
             self.constraint = self.constraint.unsqueeze(0)
        # Transpose check for safety (if saved as [H, 1])
        if self.constraint.shape[0] != 1 and self.constraint.shape[1] == 1:
             self.constraint = self.constraint.T

    def _setup_gurobi_env(self):
        # Global Gurobi settings for determinism
        gp.setParam("OutputFlag", 0) # Suppress Gurobi log output
        gp.setParam("Threads", 1)    # Use a single thread
        gp.setParam("Method", 1)     # Dual Simplex (a deterministic method)
        gp.setParam("Crossover", 0)
        gp.setParam("Seed", getattr(self.configs, "random_seed", 42))
    
    def _build_allocate_model(self):
        self.allocate_model = AllocateModel(
            self.constraint.numpy(),
            self.configs.uncertainty_quantile,
            pred_len=self.configs.pred_len, # Max horizon
            first_step_cap=getattr(self.configs, "mpc_first_step_cap", None),
            quiet=True
        )

    def evaluate(self):
        print(f"Starting MPC Evaluation on {len(self.test_loader)} batches...")
        
        total_regret = 0.0
        total_rel_regret = 0.0
        count = 0
        #TODO: logger

        # Setup Logger
        cases_dir = os.path.join(self.exp_dir, "cases_test")
        os.makedirs(cases_dir, exist_ok=True)
        logger = CaseLogger(os.path.join(cases_dir, "mpc_cases.jsonl"))
        
        for batch in tqdm(self.test_loader):
            # batch_x: Normalized history [B, Seq, 1]
            # batch_y: Normalized future [B, Pred, 1]
            batch_x, batch_y, _, _ = batch
            
            batch_x_np = batch_x.cpu().numpy()
            batch_y_np = batch_y.cpu().numpy()
            
            # Unscale ground truth future prices once, as this is used in the simulation
            batch_y_real = np.zeros_like(batch_y_np)
            for i in range(len(batch_y_np)):
                batch_y_real[i] = self.scaler.inverse_transform(batch_y_np[i].reshape(-1, 1)).flatten()

            # Run simulation for each sample in batch
            for i in range(len(batch_x_np)):
                history = batch_x_np[i].flatten()
                future = batch_y_real[i].flatten()
                
                # --- CORE CALL ---
                cost, alloc = self._run_mpc_simulation(history, future)
                # -----------------
                
                # Calculate metrics
                optimal_cost = np.min(future) # Optimal buyer buys at the minimum future price
                regret = cost - optimal_cost
                rel_regret = regret / optimal_cost if optimal_cost != 0 else 0
                
                total_regret += regret
                total_rel_regret += rel_regret
                count += 1
                
                #TODO: logger
                # Log
                logger.log({
                    "case_id": count,
                    "algo": "mpc",
                    "regret": regret,
                    "rel_regret": rel_regret,
                    "alloc": alloc,
                    "true_prices": future.tolist()
                })

        avg_regret = total_regret / count
        avg_rel_regret = total_rel_regret / count

        # --- NEW: Construct standardized result.json ---
        
        # 1. Try to get MSE/MAE from the baseline (since model is identical)
        base_metrics = {}
        try:
            base_res_path = os.path.join(self.prev_exp_dir, 'result.json')
            if os.path.exists(base_res_path):
                with open(base_res_path, 'r') as f:
                    base_metrics = json.load(f)
        except Exception:
            pass

        # 2. Construct final dictionary
        final_metrics = {
            'MSE': base_metrics.get('MSE', -1.0),       # Copy from baseline
            'MAE': base_metrics.get('MAE', -1.0),       # Copy from baseline
            'Regret': avg_regret,                       # Our new MPC result
            'Rel Regret': avg_rel_regret                # Our new MPC result
        }

        print(f"Final MPC Results: {final_metrics}")
        
        # 3. Save as standard 'result.json'
        res_path = os.path.join(self.exp_dir, 'result.json')
        with open(res_path, 'w') as f:
            json.dump(final_metrics, f, indent=4)
            
        return avg_regret

    def _run_mpc_simulation(self, history_scaled, future_unscaled):
        H = self.configs.pred_len # Should be 88
        
        # State variables
        # current_history is unscaled (real prices)
        current_history_unscaled = list(self.scaler.inverse_transform(history_scaled.reshape(-1, 1)).flatten())
        budget_remaining = 1.0
        cost_incurred = 0.0
        actions_taken = []
        
        # Loop through each step in the prediction horizon
        for t in range(H):
            # 1. Prepare history for forecasting
            # Use the last 'seq_len' real prices (the sliding window)
            hist_unscaled = np.array(current_history_unscaled[-self.configs.seq_len:]).reshape(-1, 1)
            hist_norm = self.scaler.transform(hist_unscaled)
            hist_tensor = torch.tensor(hist_norm, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # 2. Forecast
            with torch.no_grad():
                pred_norm = self.forecast_model(hist_tensor) # [1, H, 1]
            
            # Inverse transform the forecast to get real price predictions
            pred_norm_np = pred_norm.cpu().numpy().flatten()
            pred_real = self.scaler.inverse_transform(pred_norm_np.reshape(-1, 1)).flatten()
            
            # 3. Optimization (The MPC Step)
            
            # A. Determine remaining horizon and constraint
            remaining_steps = H - t
            
            if t == H - 1:
                # If it's the last step, we must spend everything
                amount_to_spend_fraction = budget_remaining
                
            else:
                # B. Get the relevant constraint vector for the remaining steps
                current_constraint = self.constraint[:, :remaining_steps].numpy()
                
                # C. Initialize "raw_action_fraction" variable
                raw_action_fraction = 0.0
                
                # D. Try to solve WITH the cap first
                try:
                    solver = AllocateModel(
                        current_constraint,
                        self.configs.uncertainty_quantile,
                        pred_len=remaining_steps,
                        first_step_cap=getattr(self.configs, "mpc_first_step_cap", None),
                        quiet=True
                    )
                    solver.setObj(pred_real[:remaining_steps])
                    sol, _ = solver.solve()
                    raw_action_fraction = float(sol[0])
                    
                except Exception:
                    # E. First Failure: Try RELAXED (Ignore Cap)
                    try:
                        solver_relaxed = AllocateModel(
                            current_constraint,
                            self.configs.uncertainty_quantile,
                            pred_len=remaining_steps,
                            first_step_cap=None, # Ignore Cap
                            quiet=True
                        )
                        solver_relaxed.setObj(pred_real[:remaining_steps])
                        sol, _ = solver_relaxed.solve()
                        raw_action_fraction = float(sol[0])
                        
                    except Exception:
                        # F. Ultimate Failure: Default to HOLD (0.0)
                        # If Gurobi fails entirely, we play it safe and spend nothing.
                        # We print a warning so you know it happened, but we keep running.
                        # print(f"Warning: Solver failed at step {t}/{H}. Holding.")
                        raw_action_fraction = 0.0
                
                # G. Cap the raw action by the remaining budget
                amount_to_spend_fraction = min(raw_action_fraction, budget_remaining)
                
            # 4. Execute (Trade)
            
            # Get the true price for the current step t
            true_price = future_unscaled[t]
            
            # The actual execution cost is the *fraction of remaining budget* times the true price
            cost_incurred += amount_to_spend_fraction * true_price
            budget_remaining -= amount_to_spend_fraction
            actions_taken.append(amount_to_spend_fraction)
            
            # 5. Update History (Slide window for the next step)
            current_history_unscaled.append(true_price)
            
        return cost_incurred, actions_taken

        


==================================================
FILE: ./core/src/utils/case_logger.py
==================================================
import json
import os

class CaseLogger:
    def __init__(self, filepath):
        self.filepath = filepath
        # Clear file if it exists so we don't append to old logs
        if os.path.exists(self.filepath):
            os.remove(self.filepath)

    def log(self, data_dict):
        """Appends a dictionary as a JSON line to the log file."""
        with open(self.filepath, "a") as f:
            f.write(json.dumps(data_dict) + "\n")


==================================================
FILE: ./core/src/models/PatchTST.py
==================================================
import torch
import torch.nn as nn

from .layers.normalization import RevIN
from .layers.embedding import TimeSeriesPatchEmbedding
from .layers.decomposition import SeasonalTrendDecomposition
from .layers.attention import ScaledDotProductAttention, MultiHeadLayer


__all__ = ['PatchTST']


class PatchTST(nn.Module):
    """(PatchTST, Nie et al, 2023) with vanilla transformer encoder
    applied with patch-based embedding and channel-independent modeling (O(L^2/S^2)).
    """

    def __init__(self, configs):
        super().__init__()

        if configs.decomp:
            self.decomp = SeasonalTrendDecomposition(configs.decomp_ksize)
            self.ptst_trend = PatchTSTBackbone(configs)
            self.ptst_seasonal = PatchTSTBackbone(configs)
        else:
            self.register_module('decomp', None)
            self.ptst = PatchTSTBackbone(configs)

    def forward(self, x_enc, *args, **kwargs):
        """
        x_enc   batch_size x seq_len x n_vars
        """
        if x_enc.dim() == 2: # Single Variable prediction
            x_enc = x_enc.unsqueeze(2)
        assert x_enc.dim() == 3
        if self.decomp is not None:
            seasonal_init, trend_init = self.decomp(x_enc)
            seasonal = self.ptst_seasonal(seasonal_init)
            trend = self.ptst_trend(trend_init)
            enc_out = seasonal + trend
        else:
            enc_out = self.ptst(x_enc)

        if enc_out.dim() == 3: # Squeeze output for Single Variable prediction
            if enc_out.size(dim=2) == 1:
                enc_out = enc_out.squeeze()
        return enc_out


class PatchTSTBackbone(nn.Module):
    """PatchTST backbone."""

    def __init__(self, configs):
        super().__init__()

        if configs.revin:
            self.revin = RevIN(configs.n_vars, configs.revin_affine, configs.revin_subtract_last)
        else:
            self.register_module('revin', None)

        self.embed = TimeSeriesPatchEmbedding(
            configs.patch_len, configs.patch_stride, configs.patch_padding,
            configs.n_vars, configs.d_model, configs.pos_enc
        )
        self.dropout_embed = nn.Dropout(configs.dropout)

        self.encoder = PatchTSTEncoder(
            configs.e_layers,
            configs.d_model, configs.n_heads,
            configs.d_ff, configs.activation,
            configs.dropout
        )

        n_patches = int((configs.seq_len - configs.patch_len) / configs.patch_stride) + 1
        n_patches += configs.patch_padding == 'end'
        in_features = n_patches * configs.d_model
        self.proj = PatchTSTPredictionHead(
            in_features, configs.n_vars, configs.pred_len,
            configs.shared_proj
        )

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        # RevIN
        if self.revin is not None:
            x = self.revin(x, mode='norm')

        # Encoder
        x_embed = self.embed(x)
        x = self.encoder(x_embed)

        # Output projection
        enc_out = self.proj(x)

        # RevIN
        if self.revin is not None:
            enc_out = self.revin(enc_out, mode='denorm')

        return enc_out


class PatchTSTEncoder(nn.Module):
    """PatchTST encoder."""

    def __init__(self, e_layers, d_model, n_heads, d_ff, activation, dropout):
        super().__init__()
        self.enc_layers = nn.ModuleList([
            PatchTSTEncoderLayer(d_model, n_heads, d_ff, activation, dropout)
            for _ in range(e_layers)
        ])

    def forward(self, x):
        """
        x   batch_size x n_vars x n_patches x d_model
        """
        _, _, L, D = x.size()

        x = x.view(-1, L, D)
        scores = None
        for enc_layer in self.enc_layers:
            x, scores = enc_layer(x, scores)

        return x


class PatchTSTPredictionHead(nn.Module):
    """PatchTST prediction head."""

    def __init__(self, in_features, n_vars, pred_len, shared_proj):
        super().__init__()
        self.n_vars = n_vars
        self.shared_proj = shared_proj

        if self.shared_proj:
            self.proj = nn.Linear(in_features, pred_len)
        else:
            self.proj = nn.ModuleList()
            for _ in range(self.n_vars):
                self.proj.append(nn.Linear(in_features, pred_len))

    def forward(self, x):
        """
        x   (batch_size * n_vars) x n_patches x d_model
        """
        _, L, D = x.size()

        x = x.view(-1, self.n_vars, L * D)
        if self.shared_proj:
            x = self.proj(x).transpose(-2, -1).contiguous()
        else:
            x = torch.stack([self.proj[ix](x[:, ix, :]) for ix in range(self.n_vars)], dim=2)

        return x


class PatchTSTEncoderLayer(nn.Module):
    """PatchTST encoder layer."""

    def __init__(self, d_model, n_heads, d_ff, activation, dropout):
        assert activation in {'relu', 'gelu'}
        super().__init__()
        self.self_attn = MultiHeadLayer(
            ScaledDotProductAttention(apply_casual_mask=False, dropout=dropout, apply_residual=True),
            d_model, n_heads
        )
        self.dropout_self_attn = nn.Dropout(dropout)
        self.norm_self_attn = nn.BatchNorm1d(d_model)

        self.mlp_ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            (nn.ReLU if activation == 'relu' else nn.GELU)(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.dropout_mlp_ffn = nn.Dropout(dropout)
        self.norm_mlp_ffn = nn.BatchNorm1d(d_model)

    def forward(self, x, prev_scores):
        """
        x               (batch_size * n_vars) x n_patches x d_model
        prev_scores     (batch_size * n_vars) x n_patches x n_patches 
        """
        # Self-attention + BatchNorm
        x_, scores = self.self_attn(x, x, x, prev_scores, return_scores=True)
        x = self.norm_self_attn(
            (x + self.dropout_self_attn(x_)).transpose(-2, -1)
        ).transpose(-2, -1).contiguous()

        # Position-wise feed-forward + BatchNorm
        x = self.norm_mlp_ffn(
            (x + self.dropout_mlp_ffn(self.mlp_ffn(x))).transpose(-2, -1)
        ).transpose(-2, -1).contiguous()
        return x, scores



==================================================
FILE: ./core/src/models/TimesNet.py
==================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from .layers.normalization import RevIN
from .layers.embedding import TimeSeriesChannelMixingEmbedding
from .layers.convolution import InceptionBlockV1


__all__ = ['TimesNet']


class TimesNet(nn.Module):
    """(TimesNet, Wu et al, 2023) applies CNNs on top of patched and unfolded 2D time series data
    to capture intra-and-inter period dependencies.
    """

    def __init__(self, configs):
        super().__init__()
        self.pred_len = configs.pred_len

        if configs.revin:
            self.revin = RevIN(configs.n_vars, configs.revin_affine, configs.revin_subtract_last)
        else:
            self.register_module('revin', None)

        # Module initialization order may affect forecasting performance (0.42 -> 0.39 on ETTh1 96)
        # Keep TimesBlock to be first initialized
        self.enc_layers = nn.ModuleList([
            TimesBlock(configs.topk, configs.d_model, configs.d_ff, configs.num_kernels, configs.activation)
            for _ in range(configs.e_layers)
        ])
        self.embed = TimeSeriesChannelMixingEmbedding(
            configs.n_vars, configs.d_model,
            configs.pos_enc, configs.temp_enc, configs.enc_freq,
            configs.dropout
        )
        self.norm = nn.LayerNorm(configs.d_model)

        self.enc_proj = nn.Linear(configs.seq_len, configs.seq_len + self.pred_len)
        self.proj = nn.Linear(configs.d_model, configs.n_vars)

    def forward(self, x_enc, x_stamp_enc, *args, **kwargs):
        """
        x_enc           batch_size x seq_len x n_vars
        x_stamp_enc     batch_size x seq_len x n_temp_feats
        """
        # RevIN
        if self.revin is not None:
            x_enc = self.revin(x_enc, mode='norm')

        # Embedding & Temporal projection
        x_embed = self.embed(x_enc, x_stamp_enc)
        x_embed = self.enc_proj(x_embed.transpose(-2, -1)).transpose(-2, -1).contiguous()

        # Encoder
        enc_out = x_embed
        for enc_layer in self.enc_layers:
            enc_out = self.norm(enc_layer(enc_out))

        # Output projection
        enc_out = self.proj(enc_out)

        # RevIN
        if self.revin is not None:
            enc_out = self.revin(enc_out, mode='denorm')

        return enc_out[:, -self.pred_len:, :] 


class TimesBlock(nn.Module):

    def __init__(self, topk, d_model, d_ff, num_kernels, activation):
        assert activation in {'relu', 'gelu'}
        super().__init__()
        self.topk = topk

        self.conv = nn.Sequential(
            InceptionBlockV1(d_model, d_ff, num_kernels),
            (nn.ReLU if activation == 'relu' else nn.GELU)(),
            InceptionBlockV1(d_ff, d_model, num_kernels)
        )

    def _topk_periodicities(self, x, topk):
        x_fft = torch.fft.rfft(x, dim=1)
        amplitude = torch.abs(x_fft).mean(0).mean(1)
        amplitude[0] = 0

        _, freq_top = torch.topk(amplitude, topk)
        freq_top = freq_top.detach().cpu().numpy()

        period_list = x.size(1) // freq_top
        period_weight = F.softmax(torch.abs(x_fft).mean(2)[:, freq_top], dim=-1)
        return period_list, period_weight

    def _multiresolution_patching(self, x, period_list):
        _, L, _ = x.size()
        x = x.transpose(-2, -1).contiguous()

        x_patch_list = []
        for period in period_list:
            x_pad = x
            pad_len = L % period
            if pad_len != 0:
                pad_len = period - pad_len
                x_pad = F.pad(x, (0, pad_len), mode='constant')

            x_patch = x_pad.unfold(dimension=-1, size=period, step=period)
            x_patch_list.append(x_patch)

        return x_patch_list

    def forward(self, x):
        """
        x   batch_size x seq_len x d_model
        """
        B, L, D = x.size()

        period_list, period_weight = self._topk_periodicities(x, self.topk)
        x_patch_list = self._multiresolution_patching(x, period_list)           

        x_out = []
        for x_patch in x_patch_list:
            x_patch = self.conv(x_patch)
            x_patch = x_patch.permute(0, 2, 3, 1).contiguous().view(B, -1, D)
            x_out.append(x_patch[:, :L, :])

        x_out = torch.stack(x_out, dim=1)
        x_ = torch.einsum('bkvl,bk->bvl', x_out, period_weight)

        x = x_ + x
        return x



==================================================
FILE: ./core/src/models/__init__.py
==================================================

from .LTSF_Linear import Linear, NLinear, DLinear
from .PatchTST import PatchTST


==================================================
FILE: ./core/src/models/layers/convolution.py
==================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from .decomposition import SeasonalTrendDecomposition


__all__ = ['MultiScaleIsometricConvolution', 'InceptionBlockV1']


class MultiScaleIsometricConvolution(nn.Module):
    """MIC layer used by (MICN, Wang et al, 2023) to consecutively capture
    local and global dependencies.    
    """

    def __init__(self, d_model, d_ff, decomp_ksizes, conv_ksizes, isoconv_ksizes, activation, dropout):
        super().__init__()
        assert len(decomp_ksizes) == len(conv_ksizes) == len(isoconv_ksizes)
        assert activation in {'relu', 'gelu'}
        self.num_scales = len(decomp_ksizes)

        # Seasonal-trend decomposition
        self.decomps = nn.ModuleList([
            SeasonalTrendDecomposition(decomp_ksize)
            for decomp_ksize in decomp_ksizes
        ])

        # Up-and-down sampling convolutions for local-global dependency modeling
        self.convs = nn.ModuleList([
            nn.Conv1d(d_model, d_model, conv_ksize, stride=conv_ksize, padding=conv_ksize//2)
            for conv_ksize in conv_ksizes
        ])
        self.isoconvs = nn.ModuleList([
            nn.Conv1d(d_model, d_model, isoconv_ksize, stride=1, padding=0)
            for isoconv_ksize in isoconv_ksizes
        ])
        self.trans_convs = nn.ModuleList([
            nn.ConvTranspose1d(d_model, d_model, conv_ksize, stride=conv_ksize, padding=0)
            for conv_ksize in conv_ksizes
        ])
        self.act_conv = nn.Tanh()
        self.norm_conv = nn.LayerNorm(d_model)
        self.dropout_conv = nn.Dropout(dropout)

        # Convolutional aggregation of multiple scales
        self.agg_conv = nn.Conv2d(d_model, d_model, (self.num_scales, 1))

        # Position-wise feed-forward
        self.mlp_ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            (nn.ReLU if activation == 'relu' else nn.GELU)(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm_mlp_ffn = nn.LayerNorm(d_model)

    def _local_global_dependency(self, x, ix):
        _, L, _ = x.size()

        # Downsampling convolution to model local dependencies
        x_ = x.transpose(-2, -1).contiguous()
        x_down = self.dropout_conv(self.act_conv(self.convs[ix](x_)))

        # Isometric convolution to model global dependencies
        x_down_pad = F.pad(x_down, (x_down.size(2) - 1, 0), mode='constant')
        x_down = self.norm_conv(
            (x_down + self.dropout_conv(self.act_conv(self.isoconvs[ix](x_down_pad)))).transpose(-2, -1)
        ).transpose(-2, -1)

        # Upsampling convolution to recover the original sequence length
        x_ = self.dropout_conv(self.act_conv(self.trans_convs[ix](x_down)))
        x_ = x_[:, :, :L].transpose(-2, -1).contiguous()
        x = self.norm_conv(x + x_)

        return x

    def forward(self, x):
        """
        x   batch_size x seq_len x d_model
        """
        # Multi-scale local-global dependency modeling
        x_list = []
        for ix in range(self.num_scales):
            x_, _ = self.decomps[ix](x)
            x_ = self._local_global_dependency(x_, ix)
            x_list.append(x_)

        # Convolutional aggregation of multi-scale results
        x = torch.stack(x_list, dim=1)
        x = self.agg_conv(x.permute(0, 3, 1, 2)).squeeze(-2).transpose(-2, -1).contiguous()

        # Position-wise feed forward
        x = self.norm_mlp_ffn(x + self.mlp_ffn(x))
        return x


class InceptionBlockV1(nn.Module):
    """Inception block used by (TimesNet, Wu et al, 2023) to jointly capture
    intra-and-inter period dependencies on reshaped time series data.
    """

    def __init__(self, in_channels, out_channels, num_kernels):
        super().__init__()

        self.conv_layers = nn.ModuleList([
            nn.Conv2d(in_channels, out_channels, kernel_size=2*ix+1, stride=1, padding=ix)
            for ix in range(num_kernels)
        ])

        self.reset_parameters()

    def reset_parameters(self):
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        x_list = []
        for conv_layer in self.conv_layers:
            x_list.append(conv_layer(x))

        x = torch.stack(x_list, dim=-1).mean(-1)
        return x



==================================================
FILE: ./core/src/models/layers/attention.py
==================================================
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from .fourier import frequency_modes


__all__ = [
    'ChannelMixingRoutedAttention',
    'FourierEnhancedAttention',
    'AutoCorrelation',
    'ProbSparseAttention',
    'ScaledDotProductAttention',
    'MultiHeadLayer'
]


class ChannelMixingRoutedAttention(nn.Module):
    """Multi-head scaled dot-product attention with router mechanism for channel-mixing.
    Router mechanism (Crossformer, Zhang et al, 2023) ensures linear complexity (O(cL))
    which is essential for attention-based channel-mixing on datasets with more variates.
    """

    def __init__(self, n_patches, n_routers, d_model, n_heads, dropout):
        super().__init__()
        self.router = nn.Parameter(torch.randn(n_patches, n_routers, d_model))
        self.sender_attn = MultiHeadLayer(
            ScaledDotProductAttention(apply_casual_mask=False, dropout=dropout),
            d_model, n_heads, mix=True 
        )
        self.receiver_attn = MultiHeadLayer(
            ScaledDotProductAttention(apply_casual_mask=False, dropout=dropout),
            d_model, n_heads, mix=True 
        )

    def forward(self, x):
        """
        x   batch_size x n_vars x n_patches x d_model 
        """
        B, V, _, D = x.size()
        _, V_R, _ = self.router.size()

        router = self.router.unsqueeze(0).repeat(B, 1, 1, 1).view(-1, V_R, D)
        x = x.transpose(1, 2).contiguous().view(-1, V, D)

        buffer = self.sender_attn(router, x, x)
        context = self.receiver_attn(x, buffer, buffer)

        context = context.view(B, -1, V, D).transpose(1, 2).contiguous()
        return context


class FourierEnhancedAttention(nn.Module):
    """FEA-f used by (FEDformer, Zhou et al, 2022) to compute attention
    in the frequency domain after frequency mode filtering. 
    """

    def __init__(self, d_model, n_heads, q_seq_len, kv_seq_len, n_modes):
        super().__init__()
        self.d_model = d_model
        self.q_modes = frequency_modes(q_seq_len, n_modes)
        self.kv_modes = frequency_modes(kv_seq_len, n_modes)

        scale = 1 / (d_model * d_model)
        d_head = d_model // n_heads
        self.weights = nn.Parameter(
            scale * torch.rand(n_heads, d_head, d_head, len(self.q_modes), dtype=torch.cfloat)
        )

    def _filter_frequency_modes(self, x_fft, modes):
        B, H, D, _ = x_fft.size()

        x_out_fft = torch.zeros(B, H, D, len(modes), dtype=torch.cfloat, device=x_fft.device)
        for ix, mode in enumerate(modes):
            if ix >= x_out_fft.size(-1) or mode >= x_fft.size(-1):
                continue
            x_out_fft[:, :, :, ix] = x_fft[:, :, :, mode]

        return x_out_fft

    def forward(self, query, key, value):
        """
        query       batch_size x n_heads x q_seq_len x d_head
        key         batch_size x n_heads x kv_seq_len x d_head
        value       batch_size x n_heads x kv_seq_len x d_head
        """
        B, H, L, D = query.size()

        query = query.transpose(-2, -1).contiguous()
        key = key.transpose(-2, -1).contiguous()

        # Transform to frequency domain
        query_fft = torch.fft.rfft(query, dim=-1)
        key_fft = torch.fft.rfft(key, dim=-1)

        # Filter frequency modes
        query_fft = self._filter_frequency_modes(query_fft, self.q_modes)
        key_fft = self._filter_frequency_modes(key_fft, self.kv_modes)

        # Compute attention in the frequency domain
        scores_fft = torch.einsum('bhdq,bhdk->bhqk', query_fft, key_fft)
        attn_fft = scores_fft.tanh()

        context_fft = torch.einsum('bhqk,bhdk->bhdq', attn_fft, key_fft)
        context_fft = torch.einsum('bhiq,hijq->bhjq', context_fft, self.weights)

        # Fill in output context frequency modes
        context_out_fft = torch.zeros(B, H, D, L // 2 + 1, dtype=torch.cfloat, device=context_fft.device)
        for ix, mode in enumerate(self.q_modes):
            if ix >= context_fft.size(-1) or mode >= context_out_fft.size(-1):
                continue
            context_out_fft[:, :, :, mode] = context_fft[:, :, :, ix]

        # Transform to time domain
        context = torch.fft.irfft(context_out_fft / self.d_model / self.d_model, n=L, dim=-1)
        return context


class AutoCorrelation(nn.Module):
    """Auto-correlation mechanism (Autoformer, Wu et al, 2022)
    conducts sub-series level aggregation based on series periodicity (O(cLlogL)).
    """

    def __init__(self, factor):
        super().__init__()
        self.factor = factor

        self.correlation = None

    def forward(self, query, key, value):
        """
        query   batch_size x n_heads x q_seq_len x d_head
        key     batch_size x n_heads x kv_seq_len x d_head
        value   batch_size x n_heads x kv_seq_len x d_head
        """
        _, _, L_Q, _ = query.size()

        key, value = self._pad_or_truncate_key_value(key, value, L_Q)
        self.correlation = self._period_based_dependency(query, key)
        context = self._time_delay_aggregation(self.correlation, value)
        return context

    def _pad_or_truncate_key_value(self, key, value, L_Q):
        """Pad or truncate key and value to query's length."""
        _, _, L_KV, _ = key.size()

        if L_Q > L_KV:
            # Pad key and value to query's length with zeros
            key = F.pad(key, (0, 0, 0, L_Q - L_KV))
            value = F.pad(value, (0, 0, 0, L_Q - L_KV))
        else:
            # Truncate key and value to query's length
            key = key[:, :, :L_Q, :]
            value = value[:, :, :L_Q, :]

        return key, value

    def _period_based_dependency(self, query, key):
        """Efficient auto-correlation computation in the Fourier domain."""
        query_fft = torch.fft.rfft(query.transpose(-2, -1), dim=-1)
        key_fft = torch.fft.rfft(key.transpose(-2, -1), dim=-1)
        correlation_fft = query_fft * torch.conj(key_fft)
        correlation = torch.fft.irfft(correlation_fft, dim=-1)
        return correlation

    def _time_delay_aggregation(self, correlation, value):
        """Aggregates shifted value based on correlation to capture period-based dependency."""
        B, H, L_KV, D = value.size()

        # Average correlation across heads and channels
        mean_correlation = correlation.mean(dim=1).mean(dim=1)

        # Sample the top clog(L_KV) delays to compute aggregation weights
        num_delays = min(self.factor * math.ceil(math.log(L_KV)), L_KV)

        if self.training:
            # Average correlation across examples in the batch during training
            _, delay_index = torch.topk(mean_correlation.mean(dim=0), num_delays, dim=-1)
            delay_correlation = mean_correlation[:, delay_index]
            weights = torch.softmax(delay_correlation, dim=-1)

            # Aggregate lagged sequences with computed weights
            context = 0.
            for ix in range(num_delays):
                delayed_value = torch.roll(value, int(-delay_index[ix]), dims=-2)
                context += torch.einsum('b,bhvd->bhvd', weights[:, ix], delayed_value)
        else:
            # Example-wise correlation ranking during inference
            weights, delay_index = torch.topk(mean_correlation, num_delays, dim=-1)
            weights = torch.softmax(weights, dim=-1)

            # Aggregate lagged sequences with computed weights
            context = 0.
            value_expand = value.repeat(1, 1, 2, 1)
            init_index = torch.arange(L_KV, device=value.device).view(1, 1, -1, 1).expand(B, H, -1, D)
            for ix in range(num_delays):
                gather_index = init_index + delay_index[:, ix].view(-1, 1, 1, 1).expand(-1, H, L_KV, D)
                delayed_value = torch.gather(value_expand, dim=-2, index=gather_index)
                context += torch.einsum('b,bhvd->bhvd', weights[:, ix], delayed_value)

        return context


class ProbSparseAttention(nn.Module):
    """ProbSparse attention (Informer, Zhou et al, 2021)
    computes attention with queries sampled based on the sparsity measurement (O(cLlogL)).
    """

    def __init__(self, apply_causal_mask, factor):
        super().__init__()
        self.apply_causal_mask = apply_causal_mask
        self.factor = factor

        self.attention = None

    def forward(self, query, key, value):
        """
        query   batch_size x n_heads x q_seq_len x d_head
        key     batch_size x n_heads x kv_seq_len x d_head
        value   batch_size x n_heads x kv_seq_len x d_head
        """
        _, _, L_Q, _ = query.size()

        attention, query_index = self._probsparse_attention_weights(query, key)
        context = self._probsparse_context(attention, value, query_index, L_Q)
        self._register_attention_weights(attention, query_index, L_Q)
        return context

    def _probsparse_attention_weights(self, query, key):
        """Computes attention weights with sampled queries."""
        B, H, L_Q, D = query.size()
        _, _, L_KV, _ = key.size()

        # Sample clog(L_KV) keys to compute the sparsity measurement
        num_keys = min(self.factor * math.ceil(math.log(L_KV)), L_KV)
        key_index = torch.randint(L_KV, (L_Q, num_keys))
        key_sample = key.unsqueeze(-3).expand(-1, -1, L_Q, -1, -1)[
            :, :, torch.arange(L_Q).unsqueeze(1), key_index, :
        ]

        # Compute the sparsity measurement
        scores = torch.einsum('bhqd,bhqkd->bhqk', query, key_sample)
        sparsity = scores.max(dim=-1)[0] - scores.sum(dim=-1) / L_KV

        # Sample the top clog(L_Q) queries based on the sparsity measurement
        num_queries = min(self.factor * math.ceil(math.log(L_Q)), L_Q)
        _, query_index = sparsity.topk(num_queries, sorted=False)

        # Compute attention scores with sampled queries
        query_sample = query[
            torch.arange(B).view(-1, 1, 1), torch.arange(H).view(1, -1, 1), query_index, :
        ]
        scores = torch.einsum('bhqd,bhkd->bhqk', query_sample, key) / math.sqrt(D)

        if self.apply_causal_mask:
            causal_mask = _probsparse_causal_mask(query_index, L_Q)
            scores = scores.masked_fill(causal_mask, -math.inf)

        attention = torch.softmax(scores, dim=-1)
        return attention, query_index

    def _probsparse_context(self, attention, value, query_index, L_Q):
        """Computes context vectors with attention weights and value."""
        B, H, L_KV, _ = value.size()

        # Initialize context vectors with cumulative sum or mean of the values
        if self.apply_causal_mask:
            # Self-attention in decoder
            assert L_KV == L_Q
            context = value.cumsum(dim=-2)
        else:
            context = value.mean(dim=-2, keepdims=True).repeat(1, 1, L_Q, 1)

        # Fill in context vectors with sparse attention
        context[
            torch.arange(B).view(-1, 1, 1), torch.arange(H).view(1, -1, 1), query_index, :
        ] = torch.matmul(attention, value)
        return context

    def _register_attention_weights(self, attention, query_index, L_Q):
        B, H, _, L_KV = attention.size()

        attention_ = torch.ones(B, H, L_Q, L_KV, device=attention.device) / L_KV
        attention_[
            torch.arange(B).view(-1, 1, 1), torch.arange(H).view(1, -1, 1), query_index, :
        ] = attention
        self.attention = attention_


class ScaledDotProductAttention(nn.Module):
    """Scaled dot-product attention (Transformer, Vaswani et al, 2017)
    with optional residual attention (Realformer, He et al, 2020) (O(L^2)).
    """

    def __init__(self, apply_casual_mask, dropout, apply_residual=False):
        super().__init__()
        self.apply_casual_mask = apply_casual_mask
        self.apply_residual = apply_residual

        self.attention = None

        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, prev_scores=None):
        """
        query           batch_size x n_heads x q_seq_len x d_head
        key             batch_size x n_heads x kv_seq_len x d_head
        value           batch_size x n_heads x kv_seq_len x d_head
        prev_scores     batch_size x n_heads x q_seq_len x kv_seq_len
        """
        _, _, L_Q, D = query.size()

        scores = torch.einsum('bhqd,bhkd->bhqk', query, key) / math.sqrt(D)

        if self.apply_residual and prev_scores is not None:
            scores += prev_scores

        if self.apply_casual_mask:
            causal_mask = _triangular_causal_mask(L_Q, query.device)
            scores = scores.masked_fill(causal_mask, -math.inf)

        self.attention = self.dropout(torch.softmax(scores, dim=-1))
        context = torch.einsum('bhqk,bhkd->bhqd', self.attention, value)
        return context, scores


class MultiHeadLayer(nn.Module):
    """Multi-head wrapper for attention and auto-correlation layers
    with optional residual from the previous layer (Realformer, He et al, 2020).
    """

    def __init__(self, layer, d_model, n_heads, mix=False):
        assert d_model % n_heads == 0
        super().__init__()
        self.layer = layer
        self.n_heads = n_heads
        self.mix = mix

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_context = nn.Linear(d_model, d_model)

    def forward(self, query, key, value, prev_scores=None, return_scores=False):
        """
        query         batch_size x q_seq_len x d_model
        key           batch_size x kv_seq_len x d_model
        value         batch_size x kv_seq_len x d_model
        prev_scores   batch_size x n_heads x q_seq_len x kv_seq_len
        """
        B, L_Q, D = query.size()
        _, L_KV, _ = key.size()

        query = self.W_q(query).view(B, L_Q, self.n_heads, -1).transpose(1, 2).contiguous()
        key = self.W_k(key).view(B, L_KV, self.n_heads, -1).transpose(1, 2).contiguous()
        value = self.W_v(value).view(B, L_KV, self.n_heads, -1).transpose(1, 2).contiguous()

        if hasattr(self.layer, 'apply_residual'):
            context, scores = self.layer(query, key, value, prev_scores)
        else:
            context = self.layer(query, key, value)

        if not self.mix:
            context = context.transpose(1, 2).contiguous()

        context = context.view(B, L_Q, D)
        context = self.W_context(context)
        return (context, scores) if return_scores else context


def _triangular_causal_mask(seq_len, device):
    """Upper triangular causal mask."""
    ones = torch.ones(seq_len, seq_len, dtype=torch.bool, device=device)
    mask = torch.triu(ones, diagonal=1)
    return mask


def _probsparse_causal_mask(query_index, seq_len):
    """Upper triangular causal mask for sampled queries."""
    B, H, _ = query_index.size()

    mask = _triangular_causal_mask(seq_len, query_index.device)
    mask = mask.view(1, 1, seq_len, seq_len).expand(B, H, -1, -1)[
        torch.arange(B).view(-1, 1, 1), torch.arange(H).view(1, -1, 1), query_index, :
    ]
    return mask



==================================================
FILE: ./core/src/models/layers/normalization.py
==================================================
import torch
import torch.nn as nn


__all__ = ['RevIN']


class RevIN(nn.Module):
    """Reversible Instance Normalization (RevIN, Kim et al, 2022)
    normalizes the look-back window and denormalizes the predicted sequence
    with statistics of the look-back window instance-wise.
    """

    def __init__(self, num_features, affine, subtract_last, eps=1e-5):
        super().__init__()
        self.num_features = num_features
        self.affine = affine
        self.subtract_last = subtract_last
        self.eps = eps

        self.center = None
        self.scale = None

        if self.affine:
            self.weight = nn.Parameter(torch.ones(self.num_features))
            self.bias = nn.Parameter(torch.zeros(self.num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)

    def forward(self, x, mode):
        """
        x   batch_size x seq_len x n_vars
        """
        assert mode in {'norm', 'denorm'}

        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)

        return x

    def _get_statistics(self, x):
        if self.subtract_last:
            self.center = x[:, [-1], :]
        else:
            self.center = torch.mean(x, dim=1, keepdim=True)
        self.scale = torch.sqrt(torch.var(x, dim=1, unbiased=False, keepdim=True) + self.eps)

    def _normalize(self, x):
        x = (x - self.center) / self.scale
        if self.affine:
            x = self.weight * x + self.bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = (x - self.bias) / (self.weight + self.eps ** 2)
        x = x * self.scale + self.center
        return x


class SeasonalLayerNorm(nn.Module):
    """Specially designed layer normalization for the seasonal component.
    Used by (Autoformer, Wu et al, 2022) and (FEDformer, Zhou et al, 2022).
    """

    def __init__(self, num_features):
        super().__init__()
        self.norm = nn.LayerNorm(num_features)

    def forward(self, x):
        """
        x   batch_size x seq_len x d_model
        """
        x = self.norm(x)    
        bias = x.mean(dim=1).unsqueeze(1)
        return x - bias



==================================================
FILE: ./core/src/models/layers/decomposition.py
==================================================
import torch
import torch.nn as nn
import torch.nn.functional as F


__all__ = ['MultiScaleHybridDecomposition', 'SeasonalTrendDecomposition']


class MultiScaleHybirdDecomposition(nn.Module):
    """Decomposes time series into seasonal and trend-cyclical components
    with composed kernels (MICN, Wang et al, 2023).
    """

    def __init__(self, kernel_sizes):
        super().__init__()
        self.decomps = nn.ModuleList([
            SeasonalTrendDecomposition(kernel_size) for kernel_size in kernel_sizes
        ])

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        seasonals = []
        trends = []
        for decomp in self.decomps:
            seasonal, trend = decomp(x)
            seasonals.append(seasonal)
            trends.append(trend)

        seasonal = sum(seasonals) / len(seasonals)
        trend = sum(trends) / len(trends)
        return seasonal, trend


class MixtureOfExpertsDecomposition(nn.Module):
    """Decomposes time series into seasonal and trend-cyclical components
    with multiple kernels weighted in a data-dependent fashion (FEDformer, Zhou et al, 2022).
    """

    def __init__(self, kernel_sizes):
        super().__init__()
        self.decomps = nn.ModuleList([
            SeasonalTrendDecomposition(kernel_size) for kernel_size in kernel_sizes
        ])
        self.linear = nn.Linear(1, len(kernel_sizes))

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        trends = []
        for decomp in self.decomps:
            _, trend = decomp(x)
            trends.append(trend)

        trends = torch.stack(trends, dim=-1)
        weights = F.softmax(self.linear(x.unsqueeze(-1)), dim=-1)
        trend = torch.sum(trends * weights, dim=-1)

        seasonal = x - trend
        return seasonal, trend


class SeasonalTrendDecomposition(nn.Module):
    """Decomposes time series into seasonal and trend-cyclical components."""

    def __init__(self, kernel_size):
        super().__init__()
        self.kernel_size = kernel_size
        self.moving_avg = nn.AvgPool1d(kernel_size, stride=1)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        front = x[:, [0], :].repeat(1, self.kernel_size - 1 - (self.kernel_size - 1) // 2, 1)
        end = x[:, [-1], :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x_pad = torch.cat([front, x, end], dim=1)

        trend = self.moving_avg(x_pad.transpose(1, 2)).transpose(1, 2)
        seasonal = x - trend
        return seasonal, trend



==================================================
FILE: ./core/src/models/layers/embedding.py
==================================================
import math

import torch
import torch.nn as nn
import torch.nn.functional as F


__all__ = ['TimeSeriesPatchEmbedding', 'TimeSeriesEmbedding']


class TimeSeriesPatchEmbedding(nn.Module):
    """Patch-based time series embedding.
        - pos_enc: Type of positional encoding.
            none: No positional encoding
            sincos: Fixed sinusoidal positional encoding
            learned: Learnable positional encoding
            learned_per_var: Learnable positional encoding for each variate
    """

    def __init__(self, patch_len, stride, padding, n_vars, d_model, pos_enc):
        assert padding in {'none', 'start', 'end'}
        assert pos_enc in {'none', 'sincos', 'learned', 'learned_per_var'}
        super().__init__()

        self.embed_patch = PatchEmbedding(patch_len, stride, padding, d_model)
        if pos_enc != 'none':
            n_vars_tmp = n_vars if pos_enc == 'learned_per_var' else None
            self.enc_pos = PositionalEncoding(d_model, pos_enc, n_vars=n_vars_tmp)
        else:
            self.register_module('enc_pos', None)


    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        x = self.embed_patch(x)
        if self.enc_pos is not None:
            x += self.enc_pos(x)

        return x


class PatchEmbedding(nn.Module):
    """Segments time series into patches and embeds patches."""

    def __init__(self, patch_len, stride, padding, d_model):
        assert padding in {'none', 'start', 'end'}
        super().__init__()

        self.patch_len = patch_len
        self.stride = stride
        self.padding = padding

        self.embed = nn.Linear(patch_len, d_model)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars 
        """
        _, L, _ = x.size()

        x = x.transpose(-2, -1).contiguous()

        # Patch segmentation
        if self.padding == 'start':
            pad_start = (L - self.patch_len) % self.stride
            if pad_start != 0:
                pad_start = self.stride - pad_start
                x = F.pad(x, (pad_start, 0), mode='replicate')
        elif self.padding == 'end':
            x = F.pad(x, (0, self.stride), mode='replicate')

        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)

        # Patch embedding
        x = self.embed(x)
        return x


class TimeSeriesChannelMixingEmbedding(nn.Module):
    """Channel-mixing-based time series embedding.
        - pos_enc: Type of positional encoding.
            none: No positional encoding
            sincos: Fixed sinusoidal positional encoding
            learned: Learnable positional encoding
            learned_per_var: Learnable positional encoding for each variate
        - temp_enc: Type of temporal encoding.
            none: No temporal encoding
            sincos: Fixed sinusoidal timestamp encoding
            learned: Learnable timestamp encoding
            learned_proj: Learnable time feature encoding based on encoded timestamp
    """

    def __init__(self, n_vars, d_model, pos_enc, temp_enc, enc_freq, dropout):
        assert pos_enc in {'none', 'sincos', 'learned', 'learned_per_var'}
        assert temp_enc in {'none', 'sincos', 'learned', 'learned_proj'}
        assert (temp_enc == 'learned_proj' and isinstance(enc_freq, str)) or \
               (temp_enc != 'learned_proj' and enc_freq is None)
        super().__init__()

        self.embed_val = ChannelMixingEmbedding(n_vars, d_model)
        if pos_enc != 'none':
            n_vars_tmp = n_vars if pos_enc == 'learned_per_var' else None
            self.enc_pos = PositionalEncoding(d_model, pos_enc, n_vars=n_vars_tmp)
        else:
            self.register_module('enc_pos', None)
        if temp_enc != 'none':
            self.enc_temp = TemporalEncoding(d_model, temp_enc, enc_freq)
        else:
            self.register_module('enc_temp', None)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, x_stamp):
        """
        x           batch_size x seq_len x n_vars
        x_stamp     batch_size x seq_len x n_temp_feats
        """
        x = self.embed_val(x)
        if self.enc_pos is not None:
            x += self.enc_pos(x)
        if self.enc_temp is not None:
            x += self.enc_temp(x_stamp)

        return self.dropout(x)


class ChannelMixingEmbedding(nn.Module):
    """1D convolutional channel-mixing value embedding."""

    def __init__(self, n_vars, d_model):
        super().__init__()
        self.embed = nn.Conv1d(
            n_vars, d_model, kernel_size=3,
            padding='same', padding_mode='circular', bias=False
        )
        nn.init.kaiming_normal_(self.embed.weight)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_vars
        """
        x = self.embed(x.transpose(1, 2)).transpose(1, 2).contiguous()
        return x


class PositionalEncoding(nn.Module):
    """Positional encoding.
        - pos_enc: Type of positional encoding.
            sincos: Fixed sinusoidal positional encoding
            learned: Learnable positional encoding
            learned_per_var: Learnable positional encoding for each variate
    """

    def __init__(self, d_model, pos_enc, max_len=1500, n_vars=None):
        assert pos_enc in {'sincos', 'learned', 'learned_per_var'}
        assert (pos_enc != 'learned_per_var' and n_vars is None) or \
               (pos_enc == 'learned_per_var' and n_vars is not None)
        super().__init__()
        self.pe = _positional_encoding(max_len, d_model, pos_enc, n_vars=n_vars)

    def forward(self, x):
        """
        x   batch_size x seq_len x d_model
            batch_size x n_vars x n_patches x d_model
        """
        L = x.size(-2)
        return self.pe[..., :L, :]


class TemporalEncoding(nn.Module):
    """Encodes timestamp information.
        - temp_enc: Type of temporal encoding.
            sincos: Fixed sinusoidal timestamp encoding
            learned: Learnable timestamp encoding
            learned_proj: Learnable time feature encoding based on encoded timestamp
    """

    def __init__(self, d_model, temp_enc, enc_freq):
        assert temp_enc in {'sincos', 'learned_embed', 'learned_proj'}
        assert (temp_enc == 'learned_proj' and isinstance(enc_freq, str)) or \
               (temp_enc != 'learned_proj' and enc_freq is None)
        super().__init__()

        if temp_enc in {'sincos', 'learned'}:
            self.enc_temp = TimeStampEncoding(d_model, temp_enc)
        elif temp_enc == 'learned_proj':
            self.enc_temp = TimeFeatureEncoding(d_model, enc_freq)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_temp_feats
        """
        return self.enc_temp(x)


class TimeStampEncoding(nn.Module):
    """Directly encodes timestamp as temporal encoding.
        - temp_enc: Type of timestamp encoding.
            sincos: Fixed sinusoidal timestamp encoding
            learned: Learnable timestamp encoding
    """

    def __init__(self, d_model, temp_enc):
        assert temp_enc in {'sincos', 'learned'}
        super().__init__()

        # The order here must align with that in
        # `data_provider.LTF_provider.LongTermForecastingDataset`
        if temp_enc == 'sincos':
            self.enc_temp = nn.ModuleList([
                nn.Embedding.from_pretrained(_positional_encoding(13, d_model, 'sincos')),   # Month
                nn.Embedding.from_pretrained(_positional_encoding(32, d_model, 'sincos')),   # Day
                nn.Embedding.from_pretrained(_positional_encoding(7, d_model, 'sincos')),    # Weekday
                nn.Embedding.from_pretrained(_positional_encoding(24, d_model, 'sincos')),   # Hour
                nn.Embedding.from_pretrained(_positional_encoding(4, d_model, 'sincos'))     # Minute
            ])
        elif temp_enc == 'learned':
            self.enc_temp = nn.ModuleList([
                nn.Embedding(13, d_model),      # Month
                nn.Embedding(32, d_model),      # Day
                nn.Embedding(7, d_model),       # Weekday
                nn.Embedding(24, d_model),      # Hour
                nn.Embedding(4, d_model)        # Minute
            ])

    def forward(self, x):
        """
        x   batch_size x seq_len x n_temp_feats
        """
        _, _, D = x.size()

        x_ = 0.
        x = x.long()
        for ix in range(D):
            x_ += self.enc_temp[ix](x[:, :, ix])

        return x_


class TimeFeatureEncoding(nn.Module):
    """Learnable time feature embedding based on encoded timestamp."""

    def __init__(self, d_model, enc_freq):
        super().__init__()
        from gluonts.time_feature import time_features_from_frequency_str
        num_features = len(time_features_from_frequency_str(enc_freq))
        self.enc_temp = nn.Linear(num_features, d_model, bias=False)

    def forward(self, x):
        """
        x   batch_size x seq_len x n_temp_feats
        """
        return self.enc_temp(x)


def _positional_encoding(max_len, d_model, pos_enc, n_vars=None):
    assert pos_enc in {'sincos', 'learned', 'learned_per_var'}
    assert (pos_enc != 'learned_per_var' and n_vars is None) or \
           (pos_enc == 'learned_per_var' and n_vars is not None)

    if pos_enc == 'sincos':
        pe = torch.zeros(max_len, d_model)

        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        requires_grad = False
    elif pos_enc == 'learned':
        pe = torch.empty(max_len, d_model)
        nn.init.uniform_(pe, a=-0.02, b=0.02)
        requires_grad = True
    elif pos_enc == 'learned_per_var':
        pe = torch.empty(n_vars, max_len, d_model)
        nn.init.normal_(pe, mean=0., std=1.)
        requires_grad = True

    return nn.Parameter(pe, requires_grad)



==================================================
FILE: ./core/src/models/layers/fourier.py
==================================================
import numpy as np
import torch
import torch.nn as nn


__all__ = ['FourierEnhancedBlock', 'frequency_modes']


class FourierEnhancedBlock(nn.Module):
    """FEB-f block used by (FEDformer, Zhou et al, 2022) to learn feature transformation
    in the frequency domain after frequency mode filtering.
    """

    def __init__(self, d_model, n_heads, seq_len, n_modes):
        super().__init__()
        self.modes = frequency_modes(seq_len, n_modes)

        scale = 1 / (d_model * d_model)
        d_head = d_model // n_heads
        self.weights = nn.Parameter(
            scale * torch.rand(n_heads, d_head, d_head, len(self.modes), dtype=torch.cfloat)
        )

    def forward(self, x, *args):
        """
        x       batch_size x n_heads x seq_len x d_head
        """
        B, H, L, D = x.size()

        x = x.transpose(-2, -1).contiguous()
        x_fft = torch.fft.rfft(x, dim=-1)

        x_out_fft = torch.zeros(B, H, D, L // 2 + 1, dtype=torch.cfloat, device=x.device)
        for ix, mode in enumerate(self.modes):
            if ix >= x_out_fft.size(-1) or mode >= x_fft.size(-1):
                continue
            x_out_fft[:, :, :, ix] = torch.einsum('bhi,hij->bhj', x_fft[:, :, :, mode], self.weights[:, :, :, ix])

        x = torch.fft.irfft(x_out_fft, n=L, dim=-1)
        return x


def frequency_modes(seq_len, n_modes):
    """Randomly select modes in the frequency domain."""
    n_modes = min(n_modes, seq_len // 2)

    all_modes = list(range(0, seq_len // 2))
    np.random.shuffle(all_modes)
    modes = all_modes[:n_modes]

    modes.sort()
    return modes



==================================================
FILE: ./core/src/models/FEDformer.py
==================================================
import torch
import torch.nn as nn

from .layers.embedding import TimeSeriesChannelMixingEmbedding
from .layers.normalization import SeasonalLayerNorm
from .layers.decomposition import MixtureOfExpertsDecomposition
from .layers.fourier import FourierEnhancedBlock
from .layers.attention import FourierEnhancedAttention, MultiHeadLayer


__all__ = ['FEDformer']


class FEDformer(nn.Module):
    """(FEDformer, Wu et al, 2022) with frequency-enhanced modules
    for frequency domain aggregation with O(LlogL) complexity.
    """

    def __init__(self, configs):
        super().__init__()
        self.label_len = configs.label_len
        self.pred_len = configs.pred_len

        self.decomp = MixtureOfExpertsDecomposition(configs.decomp_ksizes)

        self.enc_embed = TimeSeriesChannelMixingEmbedding(
            configs.n_vars, configs.d_model,
            configs.pos_enc, configs.temp_enc, configs.enc_freq,
            configs.dropout
        )
        self.dec_embed = TimeSeriesChannelMixingEmbedding(
            configs.n_vars, configs.d_model,
            configs.pos_enc, configs.temp_enc, configs.enc_freq,
            configs.dropout
        )

        self.encoder = FEDformerEncoder(
            configs.e_layers,
            configs.seq_len,
            configs.d_model, configs.n_heads, configs.n_modes,
            configs.d_ff, configs.decomp_ksizes, configs.activation,
            configs.dropout
        )
        self.decoder = FEDformerDecoder(
            configs.d_layers,
            configs.seq_len, configs.seq_len // 2 + configs.pred_len,
            configs.d_model, configs.n_heads, configs.n_modes,
            configs.d_ff, configs.n_vars, configs.decomp_ksizes, configs.activation,
            configs.dropout
        )

        self.proj_seasonal = nn.Linear(configs.d_model, configs.n_vars)

    def forward(self, x_enc, x_stamp_enc, x_dec, x_stamp_dec):
        """
        x_enc           batch_size x enc_seq_len x n_vars
        x_stamp_enc     batch_size x enc_seq_len x n_temp_feats
        x_dec           batch_size x dec_seq_len x n_vars
        x_stamp_dec     batch_size x dec_seq_len x n_temp_feats
        """
        # Decompose encoder input as start tokens for decoder
        seasonal_init, trend_init = self.decomp(x_enc)
        mean = torch.mean(x_enc, dim=1, keepdim=True).expand(-1, self.pred_len, -1)
        zeros = torch.zeros_like(x_dec[:, -self.pred_len:, :])

        # Initialize decoder input
        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)
        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)

        # Encoder
        x_enc_embed = self.enc_embed(x_enc, x_stamp_enc)
        memory = self.encoder(x_enc_embed)

        # Decoder
        x_dec_embed = self.dec_embed(seasonal_init, x_stamp_dec)
        seasonal, trend = self.decoder(x_dec_embed, trend_init, memory)

        # Output projection
        seasonal = self.proj_seasonal(seasonal[:, -self.pred_len:, :])
        dec_out = seasonal + trend[:, -self.pred_len:, :]
        return dec_out


class FEDformerEncoder(nn.Module):
    """FEDformer encoder."""

    def __init__(self, e_layers, seq_len, d_model, n_heads, n_modes, d_ff, kernel_sizes, activation, dropout):
        super().__init__()
        self.enc_layers = nn.ModuleList([
            FEDformerEncoderLayer(seq_len, d_model, n_heads, n_modes, d_ff, kernel_sizes, activation, dropout)
            for _ in range(e_layers)
        ])
        self.norm = SeasonalLayerNorm(d_model)

    def forward(self, x):
        """
        x   batch_size x enc_seq_len x d_model
        """
        for enc_layer in self.enc_layers:
            x = enc_layer(x)

        x = self.norm(x)
        return x


class FEDformerEncoderLayer(nn.Module):
    """FEDformer encoder layer with progressive decomposition architecture."""

    def __init__(self, seq_len, d_model, n_heads, n_modes, d_ff, kernel_sizes, activation, dropout):
        assert activation in {'relu', 'gelu'}
        super().__init__()
        self.self_attn = MultiHeadLayer(
            FourierEnhancedBlock(d_model, n_heads, seq_len, n_modes),
            d_model, n_heads, mix=True
        )
        self.dropout_self_attn = nn.Dropout(dropout)
        self.decomp_self_attn = MixtureOfExpertsDecomposition(kernel_sizes)

        self.conv_ffn = nn.Sequential(
            nn.Conv1d(d_model, d_ff, kernel_size=1, bias=False),
            (nn.ReLU if activation == 'relu' else nn.GELU)(),
            nn.Dropout(dropout),
            nn.Conv1d(d_ff, d_model, kernel_size=1, bias=False)
        )
        self.dropout_conv_ffn = nn.Dropout(dropout)
        self.decomp_conv_ffn = MixtureOfExpertsDecomposition(kernel_sizes)

    def forward(self, x):
        """
        x   batch_size x enc_seq_len x d_model
        """
        # Self-attention + Progressive decomposition
        x, _ = self.decomp_self_attn(
            x + self.dropout_self_attn(self.self_attn(x, x, x))
        )

        # Position-wise feed-forward + Progressive decomposition
        x, _ = self.decomp_conv_ffn(
            x + self.dropout_conv_ffn(self.conv_ffn(x.transpose(-2, -1)).transpose(-2, -1))
        )
        return x


class FEDformerDecoder(nn.Module):
    """FEDformer decoder."""

    def __init__(self, d_layers, enc_seq_len, dec_seq_len, d_model, n_heads, n_modes, d_ff, n_vars, kernel_size, activation, dropout):
        super().__init__()
        self.dec_layers = nn.ModuleList([
            FEDformerDecoderLayer(enc_seq_len, dec_seq_len, d_model, n_heads, n_modes, d_ff, n_vars, kernel_size, activation, dropout)
            for _ in range(d_layers)
        ])
        self.norm = SeasonalLayerNorm(d_model)

    def forward(self, x, trend, memory):
        """
        x           batch_size x dec_seq_len x d_model
        trend       batch_size x dec_seq_len x d_model
        memory      batch_size x enc_seq_len x d_model
        """
        for dec_layer in self.dec_layers:
            x, residual_trend = dec_layer(x, memory)
            trend = trend + residual_trend

        x = self.norm(x)
        return x, trend


class FEDformerDecoderLayer(nn.Module):
    """FEDformer decoder layer with progressive decomposition architecture."""

    def __init__(self, enc_seq_len, dec_seq_len, d_model, n_heads, n_modes, d_ff, n_vars, kernel_sizes, activation, dropout):
        assert activation in {'relu', 'gelu'}
        super().__init__()
        self.self_attn = MultiHeadLayer(
            FourierEnhancedBlock(d_model, n_heads, dec_seq_len, n_modes),
            d_model, n_heads, mix=True
        )
        self.dropout_self_attn = nn.Dropout(dropout)
        self.decomp_self_attn = MixtureOfExpertsDecomposition(kernel_sizes)

        self.cross_attn = MultiHeadLayer(
            FourierEnhancedAttention(d_model, n_heads, dec_seq_len, enc_seq_len, n_modes),
            d_model, n_heads, mix=True
        )
        self.dropout_cross_attn = nn.Dropout(dropout)
        self.decomp_cross_attn = MixtureOfExpertsDecomposition(kernel_sizes)

        self.conv_ffn = nn.Sequential(
            nn.Conv1d(d_model, d_ff, kernel_size=1, bias=False),
            (nn.ReLU if activation == 'relu' else nn.GELU)(),
            nn.Dropout(dropout),
            nn.Conv1d(d_ff, d_model, kernel_size=1, bias=False)
        )
        self.dropout_conv_ffn = nn.Dropout(dropout)
        self.decomp_conv_ffn = MixtureOfExpertsDecomposition(kernel_sizes)

        self.proj_trend = nn.Conv1d(
            d_model, n_vars, kernel_size=3,
            padding=1, padding_mode='circular',
            bias=False
        )

    def forward(self, x, memory):
        """
        x           batch_size x dec_seq_len x d_model
        memory      batch_size x enc_seq_len x d_model
        """
        # Self-attention + Progressive decomposition
        x, trend1 = self.decomp_self_attn(
            x + self.dropout_self_attn(self.self_attn(x, x, x))
        )

        # Cross-attention + Progressive decomposition
        x, trend2 = self.decomp_cross_attn(
            x + self.dropout_cross_attn(self.cross_attn(x, memory, memory))
        )

        # Position-wise feed-forward + Progressive decomposition
        x, trend3 = self.decomp_conv_ffn(
            x + self.dropout_conv_ffn(self.conv_ffn(x.transpose(-2, -1)).transpose(-2, -1))
        )

        # Synthesize trends and project
        residual_trend = trend1 + trend2 + trend3
        residual_trend = self.proj_trend(residual_trend.transpose(-2, -1)).transpose(-2, -1)
        return x, residual_trend



==================================================
FILE: ./core/src/models/Allocate.py
==================================================
import numpy as np
import gurobipy as gp
from gurobipy import GRB
from pyepo.model.grb import optGrbModel

class AllocateModel(optGrbModel):
    def __init__(
        self, 
        uncertainty, 
        uncertainty_quantile=0.5, 
        pred_len=None, 
        first_step_cap=None,
        seed=42,
        threads=1,
        method=1,
        crossover=0,
        quiet=False
        ):
        self.uncertainty = np.array(uncertainty)
        self.uncertainty_quantile = uncertainty_quantile
        self.uncertainty_bar = np.quantile(self.uncertainty, self.uncertainty_quantile)
        # print(self.uncertainty_bar)
        if pred_len is not None:
            self.pred_len = int(pred_len)
        else:
            self.pred_len = len(self.uncertainty[0])

        self.first_step_cap = None
        if first_step_cap is not None:
            self.first_step_cap = float(max(0.0, min(1.0, first_step_cap)))
        self.seed = seed
        self.threads = threads
        self.method = method
        self.crossover = crossover
        self.quiet = quiet
        super().__init__()

    def update_uncertainty(self, uncertainty):
        self.uncertainty = np.array(uncertainty)
        self.uncertainty_quantile = np.quantile(self.uncertainty, self.uncertainty_quantile)
        # print(self.uncertainty_bar)

    def _getModel(self): 
        # create a model
        optmodel = gp.Model()

        optmodel.Params.OutputFlag = 0 if self.quiet else 1
        optmodel.Params.Threads = self.threads
        optmodel.Params.Seed = self.seed
        optmodel.Params.Method = self.method
        optmodel.Params.Crossover = self.crossover
        # variables
        action = optmodel.addVars(self.pred_len, name="action", vtype=GRB.CONTINUOUS)
        # model sense
        optmodel.ModelSense = GRB.MINIMIZE
        # constraints
        optmodel.addConstr(gp.quicksum(self.uncertainty[0,i] * action[i] for i in range(self.pred_len)) <= self.uncertainty_bar)
        optmodel.addConstr(gp.quicksum(action[i] for i in range(self.pred_len)) == 1)
        if self.first_step_cap is not None and self.pred_len > 1:
            optmodel.addConstr(
                action[0] <= self.first_step_cap, 
                name="first_step_safety_cap"
            )
        # This handles cases where uncertainty vector might be larger than current pred_len
        # (Though with our slicing logic later, this is mostly a fallback)
        total_uncertainty_len = len(self.uncertainty[0])
        if self.pred_len < total_uncertainty_len:
             for i in range(self.pred_len, total_uncertainty_len):
                 optmodel.addConstr(action[i] == 0.0)
        return optmodel, action
    
    def solve(self):
        """
        Robust solve method that handles Infeasible models gracefully.
        Overrides the parent class method to prevent crashes on high-volatility data.
        """
        try:
            # Try the standard solver from the parent class (optGrbModel)
            return super().solve()
        except Exception:
            # FALLBACK: If Infeasible (Gurobi crash), return 0.0 allocation.
            # We return a list of zeros matching the prediction length.
            return [0.0] * self.pred_len, 0.0

class AllocateModelOld(optGrbModel):
    def __init__(self, uncertainty, uncertainty_bar):
        self.uncertainty = np.array(uncertainty)
        self.uncertainty_bar = uncertainty_bar
        self.pred_len = len(self.uncertainty[0])
        super().__init__()

    def _getModel(self): 
        # create a model
        optmodel = gp.Model()
        # variables
        action = optmodel.addVars(self.pred_len, name="action", vtype=GRB.CONTINUOUS)
        # model sense
        optmodel.ModelSense = GRB.MINIMIZE
        # constraints
        optmodel.addConstr(gp.quicksum(self.uncertainty[0,i] * action[i] for i in range(self.pred_len)) <= self.uncertainty_bar)
        optmodel.addConstr(gp.quicksum(action[i] for i in range(self.pred_len)) == 1)
        return optmodel, action



==================================================
FILE: ./core/src/models/LTSF_Linear.py
==================================================
import torch
import torch.nn as nn

from .layers.decomposition import SeasonalTrendDecomposition


__all__ = ['DLinear', 'NLinear', 'Linear']


class DLinear(nn.Module):
    """(LTSF-Linear, Zeng et al, 2023) with seasonal-trend decomposition."""

    def __init__(self, configs):
        super().__init__()
        self.decomp = SeasonalTrendDecomposition(configs.decomp_ksize)
        self.proj_seasonal = Linear(configs)
        self.proj_trend = Linear(configs)

    def forward(self, x_enc, *args, **kwargs):
        """
        x   batch_size x seq_len x n_vars
        """
        seasonal_init, trend_init = self.decomp(x_enc)
        seasonal_out = self.proj_seasonal(seasonal_init)
        trend_out = self.proj_trend(trend_init)
        return seasonal_out + trend_out


class NLinear(nn.Module):
    """(LTSF-Linear, Zeng et al, 2023) with subtract-last normalization."""

    def __init__(self, configs):
        super().__init__()
        self.proj = Linear(configs)

    def forward(self, x_enc, *args, **kwargs):
        """
        x   batch_size x seq_len x n_vars
        """
        seq_last = x_enc[:, [-1], :]
        x = x_enc - seq_last
        x = self.proj(x)
        x = x + seq_last
        return x


class Linear(nn.Module):
    """(LTSF-Linear, Zeng et al, 2023) models dependencies with a linear layer."""

    def __init__(self, configs):
        super().__init__()
        self.n_vars = configs.n_vars
        self.pred_len = configs.pred_len
        self.shared_proj = configs.shared_proj

        if self.shared_proj:
            self.proj = nn.Linear(configs.seq_len, self.pred_len)
        else:
            self.proj = nn.ModuleList()
            for _ in range(self.n_vars):
                self.proj.append(nn.Linear(configs.seq_len, self.pred_len))

    def forward(self, x_enc, *args, **kwargs):
        """
        x   batch_size x seq_len x n_vars
        """
        if self.shared_proj:
            x = self.proj(x_enc.transpose(-2, -1)).transpose(-2, -1).contiguous()
        else:
            x = torch.stack([self.proj[ix](x_enc[:, :, ix]) for ix in range(self.n_vars)], dim=2)

        return x



==================================================
FILE: ./core/src/forecasting/experiment_conformal.py
==================================================
import os
import json
import time

import numpy as np
import torch
import torch.nn as nn

from .data_provider import get_forecasting_loader_and_dataset

import models
from common.experiment import Experiment, EarlyStopping

class ConformalExperiment(Experiment):

    def __init__(self, configs):
        super().__init__(configs)
        self._build_dataloaders()
        self._build_model()
        self.alpha = configs.error_rate
        self.horizon= configs.pred_len

    def _build_model(self):
        self.model = getattr(models, self.configs.model)(self.configs)

        if self.configs.use_multi_gpu:
            self.model = nn.DataParallel(self.model, device_ids=self.configs.gpus)
        
        self.model.to(self.device)

    def _build_dataloaders(self):
        self.train_loader, self.train_set = get_forecasting_loader_and_dataset(self.configs, split='train', shuffle=True)
        self.val_loader, self.val_set = get_forecasting_loader_and_dataset(self.configs, split='val', shuffle=False)
        self.test_loader, self.test_set = get_forecasting_loader_and_dataset(self.configs, split='test', shuffle=False)

    def _build_criterion(self):
        if self.configs.criterion == 'MSE':
            criterion = nn.MSELoss()
        return criterion

    def train(self):
        criterion = self._build_criterion()
        optimizer, scheduler = self._build_optimizer()
        early_stopping = EarlyStopping(self.configs.patience)

        global_step = 0
        for epoch in range(self.configs.train_epochs):
            self.model.train()

            train_loss = []
            epoch_start = time.time()
            for batch in self.train_loader:
                optimizer.zero_grad()
                batch_pred, batch_true = self._forward_step(batch)
                loss = criterion(batch_pred, batch_true)

                loss.backward()
                optimizer.step()
                global_step += 1

                lr = optimizer.param_groups[0]['lr']
                self.writer.add_scalar('B.LR', lr, global_step)

                scheduler.step()

                train_loss.append(loss.item())

            train_loss = np.average(train_loss)
            val_loss, val_metrics = self.evaluate(self.val_loader, criterion)
            test_loss, test_metrics = self.evaluate(self.test_loader, criterion)

            val_mse, val_mae = val_metrics['MSE'], val_metrics['MAE']
            test_mse, test_mae = test_metrics['MSE'], test_metrics['MAE']
            print(f'Epoch {epoch + 1} | Time cost {time.time() - epoch_start:.2f}s')
            print(f'{"Train loss":<10} {train_loss:.4f} | {"Val loss":<8} {val_loss:.4f} | Test loss {test_loss:.4f}')
            print(f'{"Val MSE":<10} {val_mse:.4f} | {"Val MAE":<8} {val_mae:.4f} |')
            print(f'{"Test MSE":<10} {test_mse:.4f} | {"Test MAE":<8} {test_mae:.4f} |')

            early_stopping(val_loss)
            if early_stopping.save_model:
                self._save_checkpoint()
            if early_stopping.early_stop:
                print("Early stopping")
                break

            if self.configs.lr_scheduler == 'reduce_on_plateau':
                scheduler.step(val_loss)
            elif self.configs.lr_scheduler != 'one_cycle':
                scheduler.step()

            self.writer.add_scalar('A.Loss/a.Train', train_loss, global_step)
            self.writer.add_scalar('A.Loss/b.Val', val_loss, global_step)
            self.writer.add_scalar('A.Loss/c.Test', test_loss, global_step)

            self.writer.add_scalar('C.MSE/a.Val', val_metrics['MSE'], global_step)
            self.writer.add_scalar('C.MSE/b.Test', test_metrics['MSE'], global_step)
            self.writer.add_scalar('D.MAE/a.Val', val_metrics['MAE'], global_step)
            self.writer.add_scalar('D.MAE/b.Test', test_metrics['MAE'], global_step)

        self._load_best_checkpoint()
        return self.model
    
    @torch.no_grad()
    def evaluate(self, eval_loader, criterion=None, load_best=False, save_result=False):
        if load_best:
            self._load_best_checkpoint()

        self.model.eval()

        eval_loss, pred, true = [], [], []
        for batch in eval_loader:
            batch_pred, batch_true = self._forward_step(batch)
            if criterion is not None:
                loss = criterion(batch_pred, batch_true)
                eval_loss.append(loss.item())

            pred.append(batch_pred.cpu().numpy())
            true.append(batch_true.cpu().numpy())

        if criterion is not None:
            eval_loss = np.mean(eval_loss)
        else:
            eval_loss = None

        pred = np.concatenate(pred)
        true = np.concatenate(true)

        eval_mse = np.mean((pred -  true) ** 2).item()
        eval_mae = np.mean(np.abs(pred - true)).item()
        eval_metrics = {'MSE': eval_mse, 'MAE': eval_mae}

        if save_result:
            self._save_results(eval_metrics)
        return eval_loss, eval_metrics
   
    @torch.no_grad() 
    def calibrate(self, calib_loader, calib_set, load_best=False, save_result=True):        
        if load_best:
            self._load_best_checkpoint()

        self.model.eval()
        n_calibration = len(calib_set)
        calibration_scores = []
        for batch in calib_loader:
            batch_pred, batch_true = self._forward_step(batch)
            score = self.nonconformity(batch_pred, batch_true)
            calibration_scores.append(score)

        # [output_size, horizon, n_samples]
        self.calibration_scores = torch.vstack(calibration_scores).transpose(0, 2)

        # [horizon, output_size]
        q = min((n_calibration + 1.0) * (1 - self.alpha) / n_calibration, 1)
        corrected_q = min((n_calibration + 1.0) * (1 - self.alpha / self.horizon) / n_calibration, 1)

        self.critical_calibration_scores = self.get_critical_scores(calibration_scores=self.calibration_scores, q=q)
        self.corrected_critical_calibration_scores = self.get_critical_scores(
            calibration_scores=self.calibration_scores, q=corrected_q
        )
        if save_result:
            self._save_calib_scores(self.critical_calibration_scores.cpu().detach(), "score.pt")
            self._save_calib_scores(self.corrected_critical_calibration_scores.cpu().detach(), "score_corrected.pt")

        return self.critical_calibration_scores, self.corrected_critical_calibration_scores

    def get_critical_scores(self, calibration_scores, q):
        return torch.tensor(
            [
                [
                    torch.quantile(position_calibration_scores, q=q)
                    for position_calibration_scores in feature_calibration_scores
                ]
                for feature_calibration_scores in calibration_scores
            ]
        ).T

    def nonconformity(self, pred, true):
        return torch.nn.functional.l1_loss(pred, true, reduction="none")

    def _forward_step(self, batch):
        batch = [tensor.to(self.device) for tensor in batch]
        batch_x, batch_x_stamp, batch_y, batch_y_stamp = batch

        batch_x_dec = batch_y.clone()
        batch_x_dec[:, -self.configs.pred_len:, :] = 0

        batch_pred = self.model(batch_x, batch_x_stamp, batch_x_dec, batch_y_stamp).unsqueeze(2)

        batch_pred = batch_pred[:, -self.configs.pred_len:, :]
        batch_true = batch_y[:, -self.configs.pred_len:, :]
        return batch_pred, batch_true
    
    def _save_results(self, metrics):
        res_path = os.path.join(self.exp_dir, 'result.json')
        with open(res_path, 'w') as fout:
            json.dump(metrics, fout, indent=4)

    def _save_calib_scores(self, score, name):
        score_path = os.path.join(self.exp_dir, name)
        torch.save(score, score_path)



==================================================
FILE: ./core/src/forecasting/data_provider.py
==================================================
import os
import warnings

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader


def load_forecasting_dataset(configs, split, shuffle, interval_override=None):
    dataset = ForecastingDataset(
        os.path.join(configs.root_path, configs.data_path),
        [configs.seq_len, configs.label_len, configs.pred_len],
        configs.interval if interval_override is None else interval_override,
        split,
        task_type=configs.task_type,
        univariate=configs.univariate,
        date_col=configs.date_col,
        standardize=configs.standardize,
        enc_freq=configs.enc_freq,
    )
    loader = DataLoader(
        dataset,
        batch_size=configs.batch_size,
        shuffle=shuffle,
        num_workers=configs.num_workers
    )
    return loader

def get_forecasting_loader_and_dataset(configs, split, shuffle, interval_override=None):
    dataset = ForecastingDataset(
        os.path.join(configs.root_path, configs.data_path),
        [configs.seq_len, configs.label_len, configs.pred_len],
        configs.interval if interval_override is None else interval_override,
        split,
        task_type=configs.task_type,
        univariate=configs.univariate,
        date_col=configs.date_col,
        standardize=configs.standardize,
        enc_freq=configs.enc_freq,
    )
    loader = DataLoader(
        dataset,
        batch_size=configs.batch_size,
        shuffle=shuffle,
        num_workers=configs.num_workers
    )
    return loader, dataset

class ForecastingDataset(Dataset):

    def __init__(self, data_path, data_size, interval, split, task_type,
                 univariate=None, date_col='date', standardize=False, enc_freq=None):
        assert len(data_size) == 3
        assert split in {'train', 'val', 'test'}
        assert task_type in {'U', 'M'}
        assert isinstance(enc_freq, str) or enc_freq is None

        self.data_path = data_path
        self.seq_len, self.label_len, self.pred_len = data_size
        self.interval = interval
        self.split = split

        self.task_type = task_type
        self.univariate = univariate

        self.date_col = date_col
        self.standardize = standardize
        self.enc_freq = enc_freq

        self._load_data()

    def _load_data(self):
        df_raw = pd.read_csv(self.data_path)
        assert self.date_col in df_raw.columns
        assert (self.task_type == 'M' and self.univariate is None) or \
               (self.task_type == 'U' and self.univariate in df_raw.columns)


        if 'ETTh' in self.data_path:
            num_train = 12 * 30 * 24
            num_val = 4 * 30 * 24
            num_test = 4 * 30 * 24
        elif 'ETTm' in self.data_path:
            num_train = 12 * 30 * 24 * 4
            num_val = 4 * 30 * 24 * 4
            num_test = 4 * 30 * 24 * 4
        else:
            num_train = int(len(df_raw) * 0.7)
            num_test = int(len(df_raw) * 0.2)
            num_val = len(df_raw) - num_train - num_test

        train_end = num_train
        val_end = train_end + num_val
        test_end = val_end + num_test
        start, end = {
            'train': (0, train_end),
            'val': (train_end - self.seq_len, val_end),
            'test': (val_end - self.seq_len, test_end)
        }[self.split]

        if self.task_type == 'M':
            df_data = df_raw.drop(self.date_col, axis=1)
        elif self.task_type == 'U':
            df_data = df_raw[[self.univariate]]

        if self.standardize:
            self.scaler = StandardScaler()
            self.scaler.fit(df_data[:num_train].values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        df_stamp = df_raw[[self.date_col]].astype(str).apply(pd.to_datetime)

        df_stamp['year'] = df_stamp[self.date_col].apply(lambda row: row.year)
        df_stamp['month'] = df_stamp[self.date_col].apply(lambda row: row.month)
        df_stamp['day'] = df_stamp[self.date_col].apply(lambda row: row.day)
        df_stamp['weekday'] = df_stamp[self.date_col].apply(lambda row: row.weekday())
        df_stamp['hour'] = df_stamp[self.date_col].apply(lambda row: row.hour)
        df_stamp['minute'] = df_stamp[self.date_col].apply(lambda row: row.minute)

        stamp = df_stamp.drop([self.date_col], axis=1).values

        self.data = data[start:end]
        self.stamp = stamp[start:end]
        self.num_vars = self.data.shape[1]

    def __len__(self):
        return (len(self.data)-self.seq_len-self.pred_len)//self.interval + 1

    def __getitem__(self, index):
        if index < 0 or index >= len(self):
            raise IndexError('negative index or index out of range')

        x_start = index * self.interval
        x_end = x_start + self.seq_len
        x_data = self.data[x_start:x_end].astype(np.float32)

        # `y_data` contains the last `self.label_len` tokens of `x_data` as start tokens
        # This feature is only used by encoder-decoder models
        # The actual prediction sequence should be sliced from `y_data` by [-self.pred_len:]
        y_start = x_end - self.label_len
        y_end = y_start + self.label_len + self.pred_len
        y_data = self.data[y_start:y_end].astype(np.float32)

        x_stamp = self.stamp[x_start:x_end].astype(np.float32)
        y_stamp = self.stamp[y_start:y_end].astype(np.float32)
        return x_data, x_stamp, y_data, y_stamp

    def inverse_transform(self, data):
        if not self.standardize:
            warnings.warn('Dataset not standardized, returning as is')
            return data

        return self.scaler.inverse_transform(data)



==================================================
FILE: ./core/src/forecasting/experiment_heuristic.py
==================================================
import os
import json
import time

import numpy as np
import torch
import torch.nn as nn

from .data_provider import get_forecasting_loader_and_dataset

import models
from common.experiment import Experiment, EarlyStopping

class HeuristicAllocateExperiment(Experiment):

    def __init__(self, configs):
        super().__init__(configs)
        self.prev_exp_dir = os.path.join('output', configs.prev_exp_id)
        self._build_dataloaders()
        self._build_model()
        self._load_constraint()
        self.alpha = configs.error_rate
        self.horizon= configs.pred_len
        self.risk = configs.risk
        self.topk = configs.topk

    def _build_model(self):
        self.model = getattr(models, self.configs.model)(self.configs)

        if self.configs.use_multi_gpu:
            self.model = nn.DataParallel(self.model, device_ids=self.configs.gpus)
        
        self.model.to(self.device)
        self.load_checkpoint(model_path=os.path.join(self.prev_exp_dir, 'model.pt'))

    def _build_dataloaders(self):
        self.test_loader, self.test_set = get_forecasting_loader_and_dataset(self.configs, split='test', shuffle=False)

    def _build_criterion(self):
        if self.configs.criterion == 'MSE':
            criterion = nn.MSELoss()
        return criterion

    def _load_constraint(self):
        constraint_path = os.path.join(self.prev_exp_dir, 'score.pt')
        self.pretrained_constraint = torch.load(constraint_path).squeeze().unsqueeze(0).to("cpu").detach().numpy()

    def evaluate(self, eval_loader, scaler):
        result = self.evaluate_regret(eval_loader, scaler)
        return result

    def _cal_regret(self, y_pred, y_true, k=3):
        rank_idx = np.argpartition(y_pred, k)
        selected_value = y_true[rank_idx[:k]]
        cost = np.mean(selected_value)
        optimal = np.min(y_true)
        return cost - optimal

    @torch.no_grad()
    def evaluate_regret(self, eval_loader, scaler=None):
        self.model.eval()
        loss = 0
        rel_loss = 0
        trial = 0

        for batch in eval_loader:
            batch = [tensor.to(self.device) for tensor in batch]
            batch_pred, batch_true = self._forward_step(batch)
            batch_pred = batch_pred.squeeze().to("cpu").detach().numpy()
            batch_true = batch_true.squeeze().to("cpu").detach().numpy()
            if self.risk:
                batch_pred = batch_pred + self.pretrained_constraint
            else:
                batch_pred = batch_pred

            if scaler is not None:
                batch_pred = scaler.inverse_transform(batch_pred)
                batch_true = scaler.inverse_transform(batch_true)

            for j in range(batch_true.shape[0]):
                this_regret = self._cal_regret(batch_pred[j], batch_true[j], self.topk)
                loss += this_regret
                rel_loss += (this_regret / np.min(batch_true[j]))
            
            trial += batch_true.shape[0]

        result = {
            'abs_regret': loss / trial,
            'rel_regret': rel_loss / trial
        }

        return result
    
    def calibrate(self, calib_loader, calib_set, load_best=False):        
        if load_best:
            self._load_best_checkpoint()

        self.model.eval()
        n_calibration = len(calib_set)
        calibration_scores = []
        for batch in calib_loader:
            batch_pred, batch_true = self._forward_step(batch)
            score = self.nonconformity(batch_pred, batch_true)
            calibration_scores.append(score)

        # [output_size, horizon, n_samples]
        self.calibration_scores = torch.vstack(calibration_scores).transpose(0, 2)

        # [horizon, output_size]
        q = min((n_calibration + 1.0) * (1 - self.alpha) / n_calibration, 1)
        corrected_q = min((n_calibration + 1.0) * (1 - self.alpha / self.horizon) / n_calibration, 1)

        self.critical_calibration_scores = self.get_critical_scores(calibration_scores=self.calibration_scores, q=q)
        self.corrected_critical_calibration_scores = self.get_critical_scores(
            calibration_scores=self.calibration_scores, q=corrected_q
        )
        self._save_calib_scores(self.critical_calibration_scores.cpu().detach(), "score.pt")
        self._save_calib_scores(self.corrected_critical_calibration_scores.cpu().detach(), "score_corrected.pt")

    def get_critical_scores(self, calibration_scores, q):
        return torch.tensor(
            [
                [
                    torch.quantile(position_calibration_scores, q=q)
                    for position_calibration_scores in feature_calibration_scores
                ]
                for feature_calibration_scores in calibration_scores
            ]
        ).T

    def nonconformity(self, pred, true):
        return torch.nn.functional.l1_loss(pred, true, reduction="none")

    def _forward_step(self, batch):
        batch = [tensor.to(self.device) for tensor in batch]
        batch_x, batch_x_stamp, batch_y, batch_y_stamp = batch

        batch_x_dec = batch_y.clone()
        batch_x_dec[:, -self.configs.pred_len:, :] = 0

        batch_pred = self.model(batch_x, batch_x_stamp, batch_x_dec, batch_y_stamp).unsqueeze(2)

        batch_pred = batch_pred[:, -self.configs.pred_len:, :]
        batch_true = batch_y[:, -self.configs.pred_len:, :]
        return batch_pred, batch_true
    
    def _save_results(self, metrics):
        res_path = os.path.join(self.exp_dir, 'result.json')
        with open(res_path, 'w') as fout:
            json.dump(metrics, fout, indent=4)

    def _save_calib_scores(self, score, name):
        score_path = os.path.join(self.exp_dir, name)
        torch.save(score, score_path)



==================================================
FILE: ./core/src/run_pto.py
==================================================
import os
import shutil
import random
from argparse import Namespace, ArgumentParser

import time
import yaml
import numpy as np
import torch

from allocate.experiment_pto import PtOExperiment

def run_pto_experiment(configs):
    # Fix random seed to ensure reproducibility
    random.seed(configs.random_seed)
    np.random.seed(configs.random_seed)
    torch.manual_seed(configs.random_seed)

    # Instantiate experiment manager
    experiment = PtOExperiment(configs)
    
    # Evaluate
    print(f'{">" * 20} {"Start testing:":<15} {configs.exp_id} {"<" * 20}')
    start = time.time()
    metrics = experiment.evaluate(experiment.test_loader, experiment.test_set.scaler, save_result=True)
    print(f'Time: {(time.time() - start) / 2:.2f}')
    print(f'Test MSE: {metrics["MSE"]:.4f} | Test MAE: {metrics["MAE"]:.4f} | Test Regret: {metrics["Regret"]:.8f} | Test Rel Regret: {metrics["Rel Regret"]:.8f}')

if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()

    with open(args.config, 'r') as fin:
        configs = yaml.safe_load(fin)

    # Remove the previous experiment with the same `exp_id`.
    exp_dir = os.path.join('output', configs['Experiment']['exp_id'])
    if os.path.exists(exp_dir):
        print(f'Experiment {exp_dir} exists, delete and continue? [Y/N]', end=' ')

        response = input()
        while response not in ['Y', 'N']:
            print('Invalid choice. Choose between [Y/N]', end=' ')
            response = input()

        shutil.rmtree(exp_dir) if response == 'Y' else exit()

    os.makedirs(exp_dir)

    # Copy config file to output directory.
    config_path = os.path.join(exp_dir, 'config.yaml')
    with open(config_path, 'w') as fout:
        yaml.dump(configs, fout, indent=4, sort_keys=False)

    configs = Namespace(**{
        arg: val
        for _, args in configs.items()
        for arg, val in args.items()
    })
    
    run_pto_experiment(configs)



==================================================
FILE: ./core/src/run_heuristic.py
==================================================
import os
import shutil
import random
from argparse import Namespace, ArgumentParser

import yaml
import numpy as np
import torch

from forecasting.experiment_heuristic import HeuristicAllocateExperiment

def run_heuristic_experiment(configs):
    # Fix random seed to ensure reproducibility
    random.seed(configs.random_seed)
    np.random.seed(configs.random_seed)
    torch.manual_seed(configs.random_seed)

    # Instantiate experiment manager
    experiment = HeuristicAllocateExperiment(configs)

    # Start training and testing
    print(f'{">" * 20} {"Start testing:":<15} {configs.exp_id} {"<" * 20}')
    metrics = experiment.evaluate(
        experiment.test_loader,
        experiment.test_set.scaler
    )
    print(f'Test Abs Regret: {metrics["abs_regret"]:.10f} | Test Rel Regret: {metrics["rel_regret"]:.10f}')

if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()

    with open(args.config, 'r') as fin:
        configs = yaml.safe_load(fin)

    # Remove the previous experiment with the same `exp_id`.
    # exp_dir = os.path.join('output', configs['Experiment']['exp_id'])
    # if os.path.exists(exp_dir):
    #     print(f'Experiment {exp_dir} exists, delete and continue? [Y/N]', end=' ')

    #     response = 'Y'
    #     # response = input()
    #     # while response not in ['Y', 'N']:
    #     #     print('Invalid choice. Choose between [Y/N]', end=' ')
    #     #     response = input()

    #     shutil.rmtree(exp_dir) if response == 'Y' else exit()

    # os.makedirs(exp_dir)

    # Copy config file to output directory.
    # config_path = os.path.join(exp_dir, 'config.yaml')
    # with open(config_path, 'w') as fout:
    #     yaml.dump(configs, fout, indent=4, sort_keys=False)

    configs = Namespace(**{
        arg: val
        for _, args in configs.items()
        for arg, val in args.items()
    })

    run_heuristic_experiment(configs)



==================================================
FILE: ./core/src/run_conformal.py
==================================================
import os
import shutil
import random
from argparse import Namespace, ArgumentParser

import yaml
import numpy as np
import torch

from forecasting.conformal_experiment import ConformalExperiment

def run_conformal_experiment(configs):
    # Fix random seed to ensure reproducibility
    random.seed(configs.random_seed)
    np.random.seed(configs.random_seed)
    torch.manual_seed(configs.random_seed)

    # Instantiate experiment manager
    experiment = ConformalExperiment(configs)

    # Start training and testing
    print(f'{">" * 20} {"Start training:":<15} {configs.exp_id} {"<" * 20}')
    train_time, val_time = experiment.train()

    print(f'{">" * 20} {"Start testing:":<15} {configs.exp_id} {"<" * 20}')
    _, metrics = experiment.evaluate(
        experiment.test_loader,
        load_best=True,
        save_result=True,
        merge={'train_time': train_time, 'val_time': val_time}
    )
    print(f'Test MSE: {metrics["MSE"]:.4f} | Test MAE: {metrics["MAE"]:.4f}')

    # Start calibrating
    experiment.calibrate(experiment.val_loader, experiment.val_set, load_best=False)

if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()

    with open(args.config, 'r') as fin:
        configs = yaml.safe_load(fin)

    # Remove the previous experiment with the same `exp_id`.
    exp_dir = os.path.join('output', configs['Experiment']['exp_id'])
    if os.path.exists(exp_dir):
        print(f'Experiment {exp_dir} exists, delete and continue? [Y/N]', end=' ')

        response = 'Y'
        # response = input()
        # while response not in ['Y', 'N']:
        #     print('Invalid choice. Choose between [Y/N]', end=' ')
        #     response = input()

        shutil.rmtree(exp_dir) if response == 'Y' else exit()

    os.makedirs(exp_dir)

    # Copy config file to output directory.
    config_path = os.path.join(exp_dir, 'config.yaml')
    with open(config_path, 'w') as fout:
        yaml.dump(configs, fout, indent=4, sort_keys=False)

    configs = Namespace(**{
        arg: val
        for _, args in configs.items()
        for arg, val in args.items()
    })

    run_conformal_experiment(configs)



==================================================
FILE: ./core/src/run_pno.py
==================================================
import os
import shutil
import random
from argparse import Namespace, ArgumentParser

import yaml
import numpy as np
import torch

from allocate.experiment_pno import PnOExperiment

os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
import torch
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True, warn_only=True)

def run_experiment(configs):
    # Fix random seed to ensure reproducibility
    random.seed(configs.random_seed)
    np.random.seed(configs.random_seed)
    torch.manual_seed(configs.random_seed)

    # Instantiate experiment manager
    experiment = PnOExperiment(configs)

    # Start training and testing
    print(f'{">" * 20} {"Start training:":<15} {configs.exp_id} {"<" * 20}')
    experiment.allocate()

    # Evaluate
    print(f'{">" * 20} {"Start testing:":<15} {configs.exp_id} {"<" * 20}')
    metrics = experiment.evaluate(experiment.test_loader, experiment.test_set.scaler, load_best=True, save_result=True)
    print(f'Test MSE: {metrics["MSE"]:.4f} | Test MAE: {metrics["MAE"]:.4f} | Test Regret: {metrics["Regret"]:.8f}')

if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()

    with open(args.config, 'r') as fin:
        configs = yaml.safe_load(fin)

    # Remove the previous experiment with the same `exp_id`.
    exp_dir = os.path.join('output', configs['Experiment']['exp_id'])
    if os.path.exists(exp_dir):
        # print(f'Experiment {exp_dir} exists, delete and continue? [Y/N]', end=' ')

        response = 'Y'
        while response not in ['Y', 'N']:
            print('Invalid choice. Choose between [Y/N]', end=' ')
            response = input()

        shutil.rmtree(exp_dir) if response == 'Y' else exit()

    os.makedirs(exp_dir)

    # Copy config file to output directory.
    config_path = os.path.join(exp_dir, 'config.yaml')
    with open(config_path, 'w') as fout:
        yaml.dump(configs, fout, indent=4, sort_keys=False)

    configs = Namespace(**{
        arg: val
        for _, args in configs.items()
        for arg, val in args.items()
    })

    run_experiment(configs)



==================================================
FILE: ./core/src/common/experiment.py
==================================================
import os

import numpy as np
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

class Experiment:

    def __init__(self, configs):
        self.configs = configs
        self.device = f'cuda:{self.configs.gpu}'
        self.exp_dir = os.path.join('output', configs.exp_id)
        self.writer = SummaryWriter(log_dir=os.path.join(self.exp_dir, 'tensorboard'))

    def load_checkpoint(self, model_path):
        self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=True))

    def _build_model(self):
        raise NotImplementedError

    def _build_dataloaders(self):
        raise NotImplementedError

    def _build_optimizer(self):
        optimizer = optim.Adam(self.model.parameters(), lr=self.configs.lr)
        if self.configs.lr_scheduler == 'fixed':
            scheduler = optim.lr_scheduler.ConstantLR(optimizer, factor=1, total_iters=0)
        elif self.configs.lr_scheduler == 'exponential':
            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.configs.gamma)
        elif self.configs.lr_scheduler == 'one_cycle':
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer, max_lr=self.configs.lr,
                epochs=self.configs.train_epochs, steps_per_epoch=len(self.train_loader),
                pct_start=self.configs.pct_start
            )
        elif self.configs.lr_scheduler == 'fixed_then_exponential':
            lr_lambda = lambda epoch: 1 if epoch < 4 else 0.9 ** (epoch - 3)
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        elif self.configs.lr_scheduler == 'step_then_fixed':
            lr_lambda = lambda epoch: 0.5 ** (epoch // 2) if epoch < 11 else 1
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        elif self.configs.lr_scheduler == 'reduce_on_plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)
        return optimizer, scheduler

    def _save_checkpoint(self):
        model_path = os.path.join(self.exp_dir, 'model.pt')
        torch.save(self.model.state_dict(), model_path) 

    def _load_best_checkpoint(self):
        model_path = os.path.join(self.exp_dir, 'model.pt')
        self.load_checkpoint(model_path)


class EarlyStopping:

    def __init__(self, patience):
        self.patience = patience

        self.counter = None
        self.early_stop = None
        self.save_model = None
        self.val_loss_min = np.inf

    def __call__(self, val_loss):
        if self.val_loss_min == np.inf or val_loss < self.val_loss_min:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')
            self.val_loss_min = val_loss
            self.save_model = True
            self.counter = 0
        else:
            self.save_model = False
            self.counter += 1
            print(f'Validation loss didn\'t decrease ({self.counter} out of {self.patience})')
            if self.counter >= self.patience:
                self.early_stop = True



==================================================
FILE: ./core/src/run_mpc.py
==================================================
import os
import random
from argparse import Namespace, ArgumentParser
import yaml
import numpy as np
import torch

# 1. Enforce Determinism
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True, warn_only=True)

# 2. Import your new Experiment
from allocate.experiment_mpc import MPCExperiment

if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()

    with open(args.config, 'r') as fin:
        configs = yaml.safe_load(fin)

    # Create output directory based on exp_id
    exp_dir = os.path.join('output', configs['Experiment']['exp_id'])
    os.makedirs(exp_dir, exist_ok=True)

    # Flatten config dictionary into a Namespace object
    configs = Namespace(**{
        arg: val
        for _, args in configs.items()
        for arg, val in args.items()
    })
    
    # 3. Run the MPC Experiment
    exp = MPCExperiment(configs)
    exp.evaluate()


==================================================
FILE: ./core/summary_usdcny_graph.png
==================================================
(Binary file excluded)



==================================================
FILE: ./core/summary_coinbase_graph.png
==================================================
(Binary file excluded)



==================================================
FILE: ./core/summary_audusd_graph.png
==================================================
(Binary file excluded)



==================================================
FILE: ./core/summary_sp500.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 165.61497159878883 | 0.05043648435236931 | 0.5567296743392944 | 0.6194135546684265
0.25     | 165.61497159878883 | 0.05043648435236931 | 0.5567296743392944 | 0.6194135546684265
0.5      | 165.61497159878883 | 0.05043648435236931 | 0.5567296743392944 | 0.6194135546684265
0.75     | 165.61497159878883 | 0.05043648435236931 | 0.5567296743392944 | 0.6194135546684265
1.0      | 165.61497159878883 | 0.05043648435236931 | 0.5567296743392944 | 0.6194135546684265



==================================================
FILE: ./core/summary_coinbase.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 2427.0538689813084 | 0.05057405493699884 | 0.09530776739120483 | 0.23162753880023956
0.2      | 2399.96247989242 | 0.049881482899309756 | 0.09530776739120483 | 0.23162753880023956
0.25     | 2399.6679819564724 | 0.04982880361112196 | 0.09530776739120483 | 0.23162753880023956
0.3      | 2398.2466048627216 | 0.049781063711841635 | 0.09530776739120483 | 0.23162753880023956
0.4      | 2397.025069708653 | 0.04974398841028049 | 0.09530776739120483 | 0.23162753880023956
0.5      | 2396.9096163873137 | 0.049740169850563946 | 0.09530776739120483 | 0.23162753880023956
0.6      | 2396.9096163873137 | 0.049740169850563946 | 0.09530776739120483 | 0.23162753880023956
0.75     | 2396.9096163873137 | 0.049740169850563946 | 0.09530776739120483 | 0.23162753880023956



==================================================
FILE: ./core/configs/FEDformer/usdcny/pto.yaml
==================================================
Experiment:
    exp_id: FEDformer/usdcny_pto
    prev_exp_id: FEDformer/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes: 
    - 24
    activation: gelu
    dropout: 0.05
Allocate:
    uncertainty_quantile: 0.8
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/FEDformer/usdcny/predict.yaml
==================================================
Experiment:
    exp_id: FEDformer/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes: 
    - 24
    activation: gelu
    dropout: 0.05
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/FEDformer/usdcny/heuristic.yaml
==================================================
Experiment:
    exp_id: FEDformer/usdcny_heuristic
    prev_exp_id: FEDformer/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes: 
    - 24
    activation: gelu
    dropout: 0.05
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false


==================================================
FILE: ./core/configs/FEDformer/usdcny/pno.yaml
==================================================
Experiment:
    exp_id: FEDformer/usdcny_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes:
    - 24
    activation: gelu
    dropout: 0.05
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 20
    uncertainty_quantile: 0.8
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/FEDformer/djia/predict.yaml
==================================================
Experiment:
    exp_id: FEDformer/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 15
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes:
    - 24
    activation: gelu
    dropout: 0.05
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/FEDformer/djia/pno.yaml
==================================================
Experiment:
    exp_id: FEDformer/djia_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 15
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes:
    - 24
    activation: gelu
    dropout: 0.05
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 20
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/FEDformer/djia/heuristic.yaml
==================================================
Experiment:
    exp_id: FEDformer/djia_heuristic
    prev_exp_id: FEDformer/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 15
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes:
    - 24
    activation: gelu
    dropout: 0.05
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: false


==================================================
FILE: ./core/configs/FEDformer/djia/pto.yaml
==================================================
Experiment:
    exp_id: FEDformer/djia_pto
    prev_exp_id: FEDformer/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 15
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: FEDformer
    pos_enc: none
    temp_enc: none
    e_layers: 2
    d_layers: 1
    d_model: 512
    n_heads: 8
    n_modes: 64
    d_ff: 2048
    decomp_ksizes:
    - 24
    activation: gelu
    dropout: 0.05
Allocate:
    uncertainty_quantile: 0.8
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/DLinear/usdcny/pto.yaml
==================================================
Experiment:
    exp_id: DLinear/usdcny_pto
    prev_exp_id: DLinear/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Allocate:
    uncertainty_quantile: 0.8
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/DLinear/usdcny/pno.yaml
==================================================
Experiment:
    exp_id: DLinear/usdcny_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Allocate:
    criterion: MSE+SPO+
    criterion_weight: 1
    train_epochs: 20
    uncertainty_quantile: 0.3
    patience: 3
    batch_size: 32
    lr: 0.0003
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/DLinear/usdcny/predict.yaml
==================================================
Experiment:
    exp_id: DLinear/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/DLinear/usdcny/heuristic.yaml
==================================================
Experiment:
    exp_id: DLinear/usdcny_heuristic
    prev_exp_id: DLinear/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: false


==================================================
FILE: ./core/configs/DLinear/djia/pno.yaml
==================================================
Experiment:
    exp_id: DLinear/djia_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: d
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 6
    uncertainty_quantile: 0.8
    patience: 3
    batch_size: 32
    lr: 0.0003
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/DLinear/djia/pto.yaml
==================================================
Experiment:
    exp_id: DLinear/djia_pto
    prev_exp_id: DLinear/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: d
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Allocate:
    uncertainty_quantile: 0.3
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/DLinear/djia/heuristic.yaml
==================================================
Experiment:
    exp_id: DLinear/djia_heuristic
    prev_exp_id: DLinear/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: d
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: true


==================================================
FILE: ./core/configs/DLinear/djia/predict.yaml
==================================================
Experiment:
    exp_id: DLinear/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: d
Model:
    model: DLinear
    decomp_ksize: 25
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/nzdusd/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/nzdusd_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: nzdusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: MSE+SPO+
    criterion_weight: 0.1
    train_epochs: 3
    uncertainty_quantile: 0.8
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/nzdusd/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/nzdusd_pto
    prev_exp_id: PatchTST/nzdusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: nzdusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/nzdusd/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/nzdusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: nzdusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/nzdusd/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/nzdusd_heuristic
    prev_exp_id: PatchTST/nzdusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: nzdusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false


==================================================
FILE: ./core/configs/PatchTST/sp500/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/sp500_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: sp500
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/sp500/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/sp500_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: sp500
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00003
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/sp500/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/sp500_pto
    prev_exp_id: PatchTST/sp500_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: sp500
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 0.1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/sp500/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/sp500_heuristic
    prev_exp_id: PatchTST/sp500_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: sp500
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00003
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false


==================================================
FILE: ./core/configs/PatchTST/usdjpy/temp_mpc_0.25.yaml
==================================================
Experiment:
  exp_id: PatchTST/usdjpy_mpc_0.25cap
  prev_exp_id: PatchTST/usdjpy_pno
  mpc_first_step_cap: 0.25
Hardware:
  random_seed: 2021
  num_workers: 0
  gpu: 0
  use_multi_gpu: false
  gpus: null
Dataset:
  root_path: ../dataset
  data_path: exchange_rate_600s.csv
  task_type: U
  univariate: usdjpy
  date_col: transact_time
  n_vars: 1
  seq_len: 440
  label_len: 0
  pred_len: 88
  interval: 1
  standardize: true
  enc_freq: null
Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true
Allocate:
  criterion: SPO+
  criterion_weight: 1
  train_epochs: 100
  uncertainty_quantile: 0.3
  patience: 3
  batch_size: 32
  lr: 0.0001
  lr_scheduler: fixed
  load_prev_weights: false
Conformal:
  error_rate: 0.05



==================================================
FILE: ./core/configs/PatchTST/usdjpy/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdjpy_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdjpy
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdjpy/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdjpy_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdjpy
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdjpy/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdjpy_pto
    prev_exp_id: PatchTST/usdjpy_predict
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdjpy
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdjpy/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdjpy_heuristic
    prev_exp_id: PatchTST/usdjpy_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdjpy
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false


==================================================
FILE: ./core/configs/PatchTST/usdjpy/temp_mpc_0.3.yaml
==================================================
Experiment:
  exp_id: PatchTST/usdjpy_mpc_0.3cap
  prev_exp_id: PatchTST/usdjpy_pno
  mpc_first_step_cap: 0.3
Hardware:
  random_seed: 2021
  num_workers: 0
  gpu: 0
  use_multi_gpu: false
  gpus: null
Dataset:
  root_path: ../dataset
  data_path: exchange_rate_600s.csv
  task_type: U
  univariate: usdjpy
  date_col: transact_time
  n_vars: 1
  seq_len: 440
  label_len: 0
  pred_len: 88
  interval: 1
  standardize: true
  enc_freq: null
Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true
Allocate:
  criterion: SPO+
  criterion_weight: 1
  train_epochs: 100
  uncertainty_quantile: 0.3
  patience: 3
  batch_size: 32
  lr: 0.0001
  lr_scheduler: fixed
  load_prev_weights: false
Conformal:
  error_rate: 0.05



==================================================
FILE: ./core/configs/PatchTST/usdcny/temp_mpc_1.0.yaml
==================================================
Experiment:
  exp_id: PatchTST/usdcny_mpc_1.0cap
  prev_exp_id: PatchTST/usdcny_pno
  mpc_first_step_cap: 1.0
Hardware:
  random_seed: 2021
  num_workers: 0
  gpu: 0
  use_multi_gpu: false
  gpus: null
Dataset:
  root_path: ../dataset
  data_path: exchange_rate_600s.csv
  task_type: U
  univariate: usdcny
  date_col: transact_time
  n_vars: 1
  seq_len: 440
  label_len: 0
  pred_len: 88
  interval: 1
  standardize: true
  enc_freq: null
Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true
Allocate:
  criterion: MSE+SPO+
  criterion_weight: 1
  train_epochs: 100
  uncertainty_quantile: 0.3
  patience: 3
  batch_size: 32
  lr: 0.001
  lr_scheduler: fixed
  load_prev_weights: false
Conformal:
  error_rate: 0.05



==================================================
FILE: ./core/configs/PatchTST/usdcny/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdcny_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: MSE+SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 0.3
    patience: 3
    batch_size: 32
    lr: 0.001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdcny/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdcny_pto
    prev_exp_id: PatchTST/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: MSE+SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 0.3
    patience: 3
    batch_size: 32
    lr: 0.001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdcny/mpc.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdcny_mpc
    # This MUST match the folder name of your baseline run
    prev_exp_id: PatchTST/usdcny_pno
    # The constraint: "Spend at most 25% of remaining budget per step"
    mpc_first_step_cap: 0.25  

Hardware:
    random_seed: 2021
    num_workers: 1       
    gpu: 0
    use_multi_gpu: false
    gpus: null

Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null

Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true

Allocate:
    # Must match the quantile used in PnO baseline
    uncertainty_quantile: 0.3
    batch_size: 32


==================================================
FILE: ./core/configs/PatchTST/usdcny/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/usdcny/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/usdcny_heuristic
    prev_exp_id: PatchTST/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: true


==================================================
FILE: ./core/configs/PatchTST/coinbase/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/coinbase_pto
    prev_exp_id: PatchTST/coinbase_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: coinbase_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 0.8
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/coinbase/temp_mpc_0.7.yaml
==================================================
Experiment:
  exp_id: PatchTST/coinbase_mpc_0.7cap
  prev_exp_id: PatchTST/coinbase_pno
  mpc_first_step_cap: 0.7
Hardware:
  random_seed: 2021
  num_workers: 0
  gpu: 0
  use_multi_gpu: false
  gpus: null
Dataset:
  root_path: ../dataset
  data_path: coinbase_1hour.csv
  task_type: U
  univariate: Open
  date_col: Date
  n_vars: 1
  seq_len: 720
  label_len: 0
  pred_len: 144
  interval: 1
  standardize: true
  enc_freq: null
Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true
Allocate:
  criterion: SPO+
  criterion_weight: 1
  train_epochs: 100
  uncertainty_quantile: 0.3
  patience: 3
  batch_size: 32
  lr: 0.0001
  lr_scheduler: fixed
  load_prev_weights: false
Conformal:
  error_rate: 0.05



==================================================
FILE: ./core/configs/PatchTST/coinbase/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/coinbase_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: coinbase_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/coinbase/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/coinbase_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: coinbase_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/coinbase/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/coinbase_heuristic
    prev_exp_id: PatchTST/coinbase_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: coinbase_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: false


==================================================
FILE: ./core/configs/PatchTST/eth/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/eth_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: ETH_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/eth/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/eth_heuristic
    prev_exp_id: PatchTST/eth_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: ETH_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: true


==================================================
FILE: ./core/configs/PatchTST/eth/temp_mpc_0.6.yaml
==================================================
Experiment:
  exp_id: PatchTST/eth_mpc_0.6cap
  prev_exp_id: PatchTST/eth_pno
  mpc_first_step_cap: 0.6
Hardware:
  random_seed: 2021
  num_workers: 0
  gpu: 0
  use_multi_gpu: false
  gpus: null
Dataset:
  root_path: ../dataset
  data_path: ETH_1hour.csv
  task_type: U
  univariate: Open
  date_col: Date
  n_vars: 1
  seq_len: 720
  label_len: 0
  pred_len: 144
  interval: 1
  standardize: true
  enc_freq: null
Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true
Allocate:
  criterion: SPO+
  criterion_weight: 1
  train_epochs: 100
  uncertainty_quantile: 0.3
  patience: 3
  batch_size: 32
  lr: 0.0003
  lr_scheduler: fixed
  load_prev_weights: false
Conformal:
  error_rate: 0.05



==================================================
FILE: ./core/configs/PatchTST/eth/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/eth_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: ETH_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 0.7
    patience: 3
    batch_size: 32
    lr: 0.0003
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/eth/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/eth_pto
    prev_exp_id: PatchTST/eth_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: ETH_1hour.csv
    task_type: U
    univariate: Open
    date_col: Date
    n_vars: 1
    seq_len: 720
    label_len: 0
    pred_len: 144
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/audusd/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/audusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: audusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/audusd/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/audusd_heuristic
    prev_exp_id: PatchTST/audusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: audusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false



==================================================
FILE: ./core/configs/PatchTST/audusd/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/audusd_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: audusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: MSE+SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/audusd/test_debug.yaml
==================================================
Experiment:
  exp_id: PatchTST/audusd_mpc_debug
  prev_exp_id: PatchTST/audusd_pno  # The PnO model you just trained
  mpc_first_step_cap: 0.1

Hardware:
  random_seed: 2021
  num_workers: 0       # Critical for stability
  gpu: 0
  use_multi_gpu: false
  gpus: null

Dataset:
  root_path: ../dataset
  data_path: exchange_rate_600s.csv  # Standard for this dataset
  task_type: U
  univariate: audusd                 # Standard for this dataset
  date_col: transact_time
  n_vars: 1
  seq_len: 440
  label_len: 0
  pred_len: 88
  interval: 1
  standardize: true
  enc_freq: null

Model:
  model: PatchTST
  decomp: false
  decomp_ksize: null
  revin: true
  revin_affine: false
  revin_subtract_last: false
  patch_len: 12
  patch_stride: 6
  patch_padding: end
  pos_enc: learned
  e_layers: 3
  d_model: 16
  n_heads: 4
  d_ff: 128
  activation: gelu
  dropout: 0.3
  shared_proj: true

Allocate:
  uncertainty_quantile: 0.3
  batch_size: 32   # Critical fix


==================================================
FILE: ./core/configs/PatchTST/audusd/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/audusd_pto
    prev_exp_id: PatchTST/audusd_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: audusd
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/djia/pto.yaml
==================================================
Experiment:
    exp_id: PatchTST/djia_pto
    prev_exp_id: PatchTST/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    uncertainty_quantile: 0.1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/PatchTST/djia/pno.yaml
==================================================
Experiment:
    exp_id: PatchTST/djia_pno
Hardware:
    random_seed: 2021
    num_workers: 1
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 100
    uncertainty_quantile: 0.8
    patience: 3
    batch_size: 32
    lr: 0.0003
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/djia/predict.yaml
==================================================
Experiment:
    exp_id: PatchTST/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/PatchTST/djia/heuristic.yaml
==================================================
Experiment:
    exp_id: PatchTST/djia_heuristic
    prev_exp_id: PatchTST/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: PatchTST
    decomp: false
    decomp_ksize: null
    revin: true
    revin_affine: false
    revin_subtract_last: false
    patch_len: 12
    patch_stride: 6
    patch_padding: end
    pos_enc: learned
    e_layers: 3
    d_model: 16
    n_heads: 4
    d_ff: 128
    activation: gelu
    dropout: 0.3
    shared_proj: true
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 5
    risk: false


==================================================
FILE: ./core/configs/TimesNet/djia/heuristic.yaml
==================================================
Experiment:
    exp_id: TimesNet/djia_heuristic
    prev_exp_id: TimesNet/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: false


==================================================
FILE: ./core/configs/TimesNet/djia/pno.yaml
==================================================
Experiment:
    exp_id: TimesNet/djia_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 20
    uncertainty_quantile: 1
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/TimesNet/djia/pto.yaml
==================================================
Experiment:
    exp_id: TimesNet/djia_pto
    prev_exp_id: TimesNet/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Allocate:
    uncertainty_quantile: 0.8
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/configs/TimesNet/djia/conformal.yaml
==================================================
Experiment:
    exp_id: TimesNet/djia_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: stock_data.csv
    task_type: U
    univariate: djia
    date_col: dt
    n_vars: 1
    seq_len: 150
    label_len: 0
    pred_len: 30
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.00001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/TimesNet/usdcny/pno.yaml
==================================================
Experiment:
    exp_id: TimesNet/usdcny_pno
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Allocate:
    criterion: SPO+
    criterion_weight: 1
    train_epochs: 20
    uncertainty_quantile: 0.8
    patience: 3
    batch_size: 32
    lr: 0.0003
    lr_scheduler: fixed
    load_prev_weights: false
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/TimesNet/usdcny/predict.yaml
==================================================
Experiment:
    exp_id: TimesNet/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05


==================================================
FILE: ./core/configs/TimesNet/usdcny/heuristic.yaml
==================================================
Experiment:
    exp_id: TimesNet/usdcny_heuristic
    prev_exp_id: TimesNet/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Training:
    criterion: MSE
    train_epochs: 100
    patience: 3
    batch_size: 32
    lr: 0.0001
    lr_scheduler: fixed
Conformal:
    error_rate: 0.05
Heuristic:
    topk: 1
    risk: false


==================================================
FILE: ./core/configs/TimesNet/usdcny/pto.yaml
==================================================
Experiment:
    exp_id: TimesNet/usdcny_pto
    prev_exp_id: TimesNet/usdcny_predict
Hardware:
    random_seed: 2021
    num_workers: 2
    gpu: 0
    use_multi_gpu: false
    gpus: null
Dataset:
    root_path: ../dataset
    data_path: exchange_rate_600s.csv
    task_type: U
    univariate: usdcny
    date_col: transact_time
    n_vars: 1
    seq_len: 440
    label_len: 0
    pred_len: 88
    interval: 1
    standardize: true
    enc_freq: null
Model:
    model: TimesNet
    pos_enc: sincos
    temp_enc: none
    revin: true
    revin_affine: false
    revin_subtract_last: false
    topk: 5
    e_layers: 2
    d_model: 16
    d_ff: 32
    num_kernels: 6
    activation: gelu
    dropout: 0.1
Allocate:
    uncertainty_quantile: 0.1
    batch_size: 32
    load_prev_weights: true


==================================================
FILE: ./core/summary_djia.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 1309.063572485207 | 0.04849249436157036 | 0.3551647961139679 | 0.48154720664024353
0.25     | 1309.063572485207 | 0.04849249436157036 | 0.3551647961139679 | 0.48154720664024353
0.5      | 1309.063572485207 | 0.04849249436157036 | 0.3551647961139679 | 0.48154720664024353
0.75     | 1309.063572485207 | 0.04849249436157036 | 0.3551647961139679 | 0.48154720664024353
1.0      | 1309.063572485207 | 0.04849249436157036 | 0.3551647961139679 | 0.48154720664024353



==================================================
FILE: ./core/summary_usdjpy.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 0.005858537466114381 | 0.0037506117106857776 | 0.12003812938928604 | 0.27955660223960876
0.2      | 0.005858537466114381 | 0.0037506117106857776 | 0.12003812938928604 | 0.27955660223960876



==================================================
FILE: ./core/summary_usdcny.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 0.0034386831617327076 | 0.0004754966945323532 | 0.0060256170108914375 | 0.05091505125164986
0.25     | 0.003285269277900379 | 0.0004543204776192754 | 0.0060256170108914375 | 0.05091505125164986
0.5      | 0.003219990084704034 | 0.0004452927674313593 | 0.0060256170108914375 | 0.05091505125164986
0.75     | 0.003243180249916217 | 0.00044850787918513144 | 0.0060256170108914375 | 0.05091505125164986



==================================================
FILE: ./core/graph.py
==================================================
import matplotlib
matplotlib.use('Agg')  # Use a non-interactive backend for saving only
import matplotlib.pyplot as plt
import pandas as pd
import sys
import os

def process_and_save_graph(file_path):
    # 1. Check if file exists
    if not os.path.exists(file_path):
        print(f"Error: The file '{file_path}' does not exist.")
        return

    # 2. Load the data
    try:
        # sep='|' handles the pipe separators
        # skiprows=[1] ignores the '-------' line
        # skipinitialspace=True handles whitespace around data
        df = pd.read_csv(file_path, sep='|', skiprows=[1], skipinitialspace=True)
        
        # Clean whitespace from column names and string data
        df.columns = [c.strip() for c in df.columns]
        
        # Sort by Cap_Size to ensure the lines connect correctly
        df = df.sort_values(by='Cap_Size').reset_index(drop=True)
        
    except Exception as e:
        print(f"Error parsing the file: {e}")
        return

    # 3. Setup the visualization
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(f'Analysis for: {os.path.basename(file_path)}', fontsize=16, fontweight='bold')

    metrics = ['Abs_Regret', 'Rel_Regret', 'MSE', 'MAE']
    colors = ['#1f77b4', '#2ca02c', '#d62728', '#9467bd']
    markers = ['o', 's', '^', 'd']

    for i, ax in enumerate(axes.flat):
        metric = metrics[i]
        ax.plot(df['Cap_Size'], df[metric], marker=markers[i], color=colors[i], 
                linestyle='-', linewidth=2, markersize=8)
        ax.set_title(f'{metric} vs Cap_Size', fontsize=12, pad=10)
        ax.set_xlabel('Cap_Size')
        ax.set_ylabel('Value')
        ax.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    # 4. Generate the output filename
    # os.path.splitext splits "data.txt" into ("data", ".txt")
    base_name, _ = os.path.splitext(file_path)
    output_filename = f"{base_name}_graph.png"

    # 5. Save and Close
    plt.savefig(output_filename, dpi=300) # dpi=300 for high quality
    plt.close(fig) 
    print(f"Successfully saved graph to: {output_filename}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        process_and_save_graph(sys.argv[1])
    else:
        path = input("Please enter the path to your .txt file: ")
        process_and_save_graph(path)


==================================================
FILE: ./core/run_sensitivity.sh
==================================================
#!/bin/bash

# ==============================================================================
# FAST SENSITIVITY ANALYSIS (MPC ONLY)
# ==============================================================================

# 1. Define Datasets (Excluding usdcny as you already have it)
DATASETS=("usdcny")

# 2. Define Budget Caps to test
CAPS=(0.1 0.25 0.5 0.75 1.0)

PYTHON_EXEC="python"

# Helper to generate config
generate_yaml() {
    local dataset=$1
    local cap=$2
    local output_file=$3
    
    $PYTHON_EXEC -c "
import yaml
import os

pno_config_path = 'configs/PatchTST/${dataset}/pno.yaml'
with open(pno_config_path, 'r') as f:
    config = yaml.safe_load(f)

# Modify for MPC
config['Experiment']['exp_id'] = f'PatchTST/${dataset}_mpc_${cap}cap'
config['Experiment']['prev_exp_id'] = f'PatchTST/${dataset}_pno'
config['Experiment']['mpc_first_step_cap'] = float(${cap})

# Ensure critical hardware/allocation settings
if 'Hardware' not in config: config['Hardware'] = {}
config['Hardware']['num_workers'] = 0 
if 'Allocate' not in config: config['Allocate'] = {}
config['Allocate']['batch_size'] = 32
config['Allocate']['uncertainty_quantile'] = 0.3 

with open('${output_file}', 'w') as f:
    yaml.dump(config, f, sort_keys=False)
"
}

# Helper to extract metrics
extract_metric() {
    local json_file=$1
    local key=$2
    if [ -f "$json_file" ]; then
        $PYTHON_EXEC -c "import json; print(json.load(open('$json_file'))['$key'])" 2>/dev/null
    else
        echo "N/A"
    fi
}

# ==============================================================================
# MAIN LOOP
# ==============================================================================

for DATASET in "${DATASETS[@]}"; do
    echo "=================================================================="
    echo "PROCESSING DATASET: $DATASET"
    echo "=================================================================="
    
    SUMMARY_FILE="summary_${DATASET}.txt"
    # Create header if file doesn't exist
    if [ ! -f "$SUMMARY_FILE" ]; then
        echo "Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE" > "$SUMMARY_FILE"
        echo "---------------------------------------------------------" >> "$SUMMARY_FILE"
    fi

    # Check if PnO model exists (Safety Check)
    PNO_MODEL="output/PatchTST/${DATASET}_pno/model.pt"
    if [ ! -f "$PNO_MODEL" ]; then
        echo "[ERROR] PnO model not found for $DATASET! Skipping..."
        continue
    fi

    # Run MPC Loops
    for CAP in "${CAPS[@]}"; do
        echo "  > Running MPC with Cap: $CAP"
        
        TEMP_CONFIG="configs/PatchTST/${DATASET}/temp_mpc_${CAP}.yaml"
        generate_yaml "$DATASET" "$CAP" "$TEMP_CONFIG"
        
        # Run MPC
        $PYTHON_EXEC src/run_mpc.py --config "$TEMP_CONFIG"
        
        # Collect Results
        RESULT_JSON="output/PatchTST/${DATASET}_mpc_${CAP}cap/result.json"
        
        if [ -f "$RESULT_JSON" ]; then
            REGRET=$(extract_metric "$RESULT_JSON" "Regret")
            REL_REG=$(extract_metric "$RESULT_JSON" "Rel Regret")
            MSE=$(extract_metric "$RESULT_JSON" "MSE")
            MAE=$(extract_metric "$RESULT_JSON" "MAE")
            
            # Format nicely
            printf "%-8s | %-10s | %-10s | %-5s | %-5s\n" "$CAP" "$REGRET" "$REL_REG" "$MSE" "$MAE" >> "$SUMMARY_FILE"
            echo "    [DONE] Regret: $REGRET"
        else
            echo "    [FAIL] No result.json found!"
        fi
        
        rm "$TEMP_CONFIG"
    done
    
    echo "Finished $DATASET. Results in $SUMMARY_FILE"
    echo ""
done


==================================================
FILE: ./core/summary_audusd.txt
==================================================
Cap_Size | Abs_Regret | Rel_Regret | MSE | MAE
---------------------------------------------------------
0.1      | 0.0022437337525979214 | 0.00339609549375925 | 0.03384649008512497 | 0.1410181224346161
0.2      | 0.0022375894410122175 | 0.003387682718131191 | 0.03384649008512497 | 0.1410181224346161
0.25     | 0.002253691147726974 | 0.0034123159123107425 | 0.03384649008512497 | 0.1410181224346161
0.3      | 0.0022648473279608997 | 0.0034294765124302656 | 0.03384649008512497 | 0.1410181224346161
0.4      | 0.0022631028323247503 | 0.003427131203016948 | 0.03384649008512497 | 0.1410181224346161
0.5      | 0.002248510268477488 | 0.003405219249732793 | 0.03384649008512497 | 0.1410181224346161
0.6      | 0.0022462118254154406 | 0.0034018689239465123 | 0.03384649008512497 | 0.1410181224346161
0.75     | 0.002243775727072584 | 0.0033983439166763944 | 0.03384649008512497 | 0.1410181224346161
0.7      | 0.002244724218902123 | 0.003399731146988619 | 0.03384649008512497 | 0.1410181224346161
0.8      | 0.002242234484873769 | 0.0033960601257602275 | 0.03384649008512497 | 0.1410181224346161
0.9      | 0.0022410114699778524 | 0.0033942573045762873 | 0.03384649008512497 | 0.1410181224346161
1.0      | 0.00224049777866158 | 0.003393484699666944 | 0.03384649008512497 | 0.1410181224346161



==================================================
FILE: ./dump_project.sh
==================================================
#!/usr/bin/env bash
set -e

OUTPUT_FILE="project_dump.txt"

# Empty or create the output file
: > "$OUTPUT_FILE"

# Find all regular files, excluding:
# 1. .git directory
# 2. core/output directory (too big)
# 3. __pycache__ directories (unnecessary binaries)
# 4. The output file itself
find . \
  -path './.git' -prune -o \
  -path './core/output' -prune -o \
  -name '__pycache__' -prune -o \
  -type f ! -name "$OUTPUT_FILE" -print0 |
while IFS= read -r -d '' file; do
  echo "==================================================" >> "$OUTPUT_FILE"
  echo "FILE: $file" >> "$OUTPUT_FILE"
  echo "==================================================" >> "$OUTPUT_FILE"
  
  # Check for binary extensions (PNG, PYC, etc)
  if [[ "$file" == *.png || "$file" == *.pyc || "$file" == *.jpg ]]; then
    echo "(Binary file excluded)" >> "$OUTPUT_FILE"

  # Check for dataset folder
  elif [[ "$file" == ./dataset/* ]]; then
    echo "(Showing first 10 lines only – dataset truncated)" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    head -n 10 "$file" >> "$OUTPUT_FILE"

  # Standard text files
  else
    cat "$file" >> "$OUTPUT_FILE"
  fi

  echo -e "\n\n" >> "$OUTPUT_FILE"
done

echo "Done. Written to $OUTPUT_FILE"


